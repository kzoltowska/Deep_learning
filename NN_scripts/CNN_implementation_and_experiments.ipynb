{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b711a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Activation\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "# setting learning parameters\n",
    "batch_size = 128 # minibatch size\n",
    "num_classes = 10\n",
    "epochs = 24 # how many full passes through the full training set\n",
    "\n",
    "# each minibatch process results in a gradient descent step and model parameter updates\n",
    "\n",
    "# input image dimensions\n",
    "img_rows, img_cols = 28, 28\n",
    "\n",
    "# the data, split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c20298d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if K.image_data_format() == 'channels_first': #K.image_data_format() returns a string telling us how keras wants the output for the tensorflow backend\n",
    "    # we need to reformat the data accordingly \n",
    "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
    "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255 #scaling of the data \n",
    "x_test /= 255\n",
    "\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes) #one hot encoding mapping of the label values\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb1db7a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model parameters = 1199882\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 24, 24, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 9216)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               1179776   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 1,199,882\n",
      "Trainable params: 1,199,882\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-08 18:54:34.583493: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2026-02-08 18:54:34.589434: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2026-02-08 18:54:34.602074: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    }
   ],
   "source": [
    "# building model with keras, sequentially adding layers \n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3), # first argument is number of filters and then kernel size\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25)) #25% probability of dropping out the output\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu')) #128 nodes\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "#compile the model, adding loss function, optimiser and metrics to report during training\n",
    "model.compile(loss=keras.losses.categorical_crossentropy, #multiclass version of binary cross-entropy\n",
    "              optimizer=keras.optimizers.Adam(), #Adam changes learning rate during training\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print()\n",
    "print(\"Model parameters = %d\" % model.count_params())\n",
    "print()\n",
    "print(model.summary())\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37a8cc65",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-08 18:55:57.612133: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2026-02-08 18:55:57.619091: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2188805000 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/24\n",
      "469/469 [==============================] - 77s 161ms/step - loss: 0.4805 - accuracy: 0.8496 - val_loss: 0.0555 - val_accuracy: 0.9816\n",
      "Epoch 2/24\n",
      "469/469 [==============================] - 78s 166ms/step - loss: 0.0853 - accuracy: 0.9740 - val_loss: 0.0385 - val_accuracy: 0.9875\n",
      "Epoch 3/24\n",
      "469/469 [==============================] - 86s 184ms/step - loss: 0.0639 - accuracy: 0.9805 - val_loss: 0.0353 - val_accuracy: 0.9882\n",
      "Epoch 4/24\n",
      "469/469 [==============================] - 87s 186ms/step - loss: 0.0508 - accuracy: 0.9838 - val_loss: 0.0300 - val_accuracy: 0.9899\n",
      "Epoch 5/24\n",
      "469/469 [==============================] - 79s 169ms/step - loss: 0.0432 - accuracy: 0.9862 - val_loss: 0.0300 - val_accuracy: 0.9903\n",
      "Epoch 6/24\n",
      "469/469 [==============================] - 84s 179ms/step - loss: 0.0353 - accuracy: 0.9889 - val_loss: 0.0269 - val_accuracy: 0.9917\n",
      "Epoch 7/24\n",
      "469/469 [==============================] - 90s 192ms/step - loss: 0.0307 - accuracy: 0.9901 - val_loss: 0.0252 - val_accuracy: 0.9921\n",
      "Epoch 8/24\n",
      "469/469 [==============================] - 77s 163ms/step - loss: 0.0294 - accuracy: 0.9910 - val_loss: 0.0280 - val_accuracy: 0.9924\n",
      "Epoch 9/24\n",
      "469/469 [==============================] - 80s 171ms/step - loss: 0.0249 - accuracy: 0.9920 - val_loss: 0.0286 - val_accuracy: 0.9924\n",
      "Epoch 10/24\n",
      "469/469 [==============================] - 78s 166ms/step - loss: 0.0237 - accuracy: 0.9925 - val_loss: 0.0299 - val_accuracy: 0.9915\n",
      "Epoch 11/24\n",
      "469/469 [==============================] - 79s 169ms/step - loss: 0.0215 - accuracy: 0.9935 - val_loss: 0.0272 - val_accuracy: 0.9929\n",
      "Epoch 12/24\n",
      "469/469 [==============================] - 82s 176ms/step - loss: 0.0201 - accuracy: 0.9931 - val_loss: 0.0284 - val_accuracy: 0.9930\n",
      "Epoch 13/24\n",
      "469/469 [==============================] - 79s 169ms/step - loss: 0.0176 - accuracy: 0.9939 - val_loss: 0.0286 - val_accuracy: 0.9915\n",
      "Epoch 14/24\n",
      "469/469 [==============================] - 88s 188ms/step - loss: 0.0179 - accuracy: 0.9941 - val_loss: 0.0302 - val_accuracy: 0.9919\n",
      "Epoch 15/24\n",
      "469/469 [==============================] - 83s 177ms/step - loss: 0.0163 - accuracy: 0.9947 - val_loss: 0.0290 - val_accuracy: 0.9920\n",
      "Epoch 16/24\n",
      "469/469 [==============================] - 81s 172ms/step - loss: 0.0151 - accuracy: 0.9949 - val_loss: 0.0289 - val_accuracy: 0.9924\n",
      "Epoch 17/24\n",
      "469/469 [==============================] - 81s 173ms/step - loss: 0.0147 - accuracy: 0.9951 - val_loss: 0.0262 - val_accuracy: 0.9927\n",
      "Epoch 18/24\n",
      "469/469 [==============================] - 78s 166ms/step - loss: 0.0137 - accuracy: 0.9952 - val_loss: 0.0288 - val_accuracy: 0.9926\n",
      "Epoch 19/24\n",
      "469/469 [==============================] - 83s 177ms/step - loss: 0.0120 - accuracy: 0.9957 - val_loss: 0.0285 - val_accuracy: 0.9926\n",
      "Epoch 20/24\n",
      "469/469 [==============================] - 92s 197ms/step - loss: 0.0108 - accuracy: 0.9961 - val_loss: 0.0320 - val_accuracy: 0.9923\n",
      "Epoch 21/24\n",
      "469/469 [==============================] - 83s 179ms/step - loss: 0.0126 - accuracy: 0.9958 - val_loss: 0.0303 - val_accuracy: 0.9927\n",
      "Epoch 22/24\n",
      "469/469 [==============================] - 81s 172ms/step - loss: 0.0110 - accuracy: 0.9959 - val_loss: 0.0294 - val_accuracy: 0.9932\n",
      "Epoch 23/24\n",
      "469/469 [==============================] - 81s 172ms/step - loss: 0.0103 - accuracy: 0.9969 - val_loss: 0.0300 - val_accuracy: 0.9935\n",
      "Epoch 24/24\n",
      "469/469 [==============================] - 88s 187ms/step - loss: 0.0125 - accuracy: 0.9959 - val_loss: 0.0311 - val_accuracy: 0.9934\n",
      "Test loss: 0.031112566590309143\n",
      "Test accuracy: 0.993399977684021\n"
     ]
    }
   ],
   "source": [
    "#training the model\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test)) #here we use all test data as validation, normally we should keep some as final tes data\n",
    "score = model.evaluate(x_test, y_test, verbose=0) #here normally we would use test data that was not used in fit \n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n",
    "model.save(\"/home/kasia/Deep_learning/Models/mnist_cnn_base_model.keras\") #keras score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d2ab62",
   "metadata": {},
   "outputs": [],
   "source": [
    "#some notes from experiments\n",
    "# more convolutional layers -more parameters, but sometimes just minor performance increase, this can be mitigated by adding extra pooling layer\n",
    "# kernel size also matters\n",
    "# dense layers are most expensive in terms of number of parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e8c4f521",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exp  2: test loss: 0.020032132044434547 test accuracy: 0.9950000047683716\n"
     ]
    }
   ],
   "source": [
    "# optimised model \n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(5, 5),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), activation='relu')) # adding extra conv layers followed by maxpooling\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adam(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=0,\n",
    "          validation_data=(x_test[:1000], y_test[:1000])) # here also a split to real test and validation data\n",
    "score = model.evaluate(x_test[1000:], y_test[1000:], verbose=0)\n",
    "print('Exp  2: test loss:', score[0], 'test accuracy:', score[1])\n",
    "model.save(\"/home/kasia/Deep_learning/Models/mnist_cnn/mnist_cnn_exp2_3_all_model.keras\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
