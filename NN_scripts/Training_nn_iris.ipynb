{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d63fd6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35bba391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.71673244\n",
      "Iteration 2, loss = 0.71547750\n",
      "Iteration 3, loss = 0.71444850\n",
      "Iteration 4, loss = 0.71326457\n",
      "Iteration 5, loss = 0.71211383\n",
      "Iteration 6, loss = 0.71121426\n",
      "Iteration 7, loss = 0.71018645\n",
      "Iteration 8, loss = 0.70919306\n",
      "Iteration 9, loss = 0.70833484\n",
      "Iteration 10, loss = 0.70741872\n",
      "Iteration 11, loss = 0.70648315\n",
      "Iteration 12, loss = 0.70572659\n",
      "Iteration 13, loss = 0.70486004\n",
      "Iteration 14, loss = 0.70421412\n",
      "Iteration 15, loss = 0.70339094\n",
      "Iteration 16, loss = 0.70275694\n",
      "Iteration 17, loss = 0.70205421\n",
      "Iteration 18, loss = 0.70139669\n",
      "Iteration 19, loss = 0.70078694\n",
      "Iteration 20, loss = 0.70022103\n",
      "Iteration 21, loss = 0.69962949\n",
      "Iteration 22, loss = 0.69915460\n",
      "Iteration 23, loss = 0.69860972\n",
      "Iteration 24, loss = 0.69814754\n",
      "Iteration 25, loss = 0.69768496\n",
      "Iteration 26, loss = 0.69732317\n",
      "Iteration 27, loss = 0.69681502\n",
      "Iteration 28, loss = 0.69639758\n",
      "Iteration 29, loss = 0.69611351\n",
      "Iteration 30, loss = 0.69575836\n",
      "Iteration 31, loss = 0.69531930\n",
      "Iteration 32, loss = 0.69507516\n",
      "Iteration 33, loss = 0.69482981\n",
      "Iteration 34, loss = 0.69447328\n",
      "Iteration 35, loss = 0.69424412\n",
      "Iteration 36, loss = 0.69399809\n",
      "Iteration 37, loss = 0.69373375\n",
      "Iteration 38, loss = 0.69355138\n",
      "Iteration 39, loss = 0.69327784\n",
      "Iteration 40, loss = 0.69312087\n",
      "Iteration 41, loss = 0.69292730\n",
      "Iteration 42, loss = 0.69274411\n",
      "Iteration 43, loss = 0.69256888\n",
      "Iteration 44, loss = 0.69241456\n",
      "Iteration 45, loss = 0.69226027\n",
      "Iteration 46, loss = 0.69214197\n",
      "Iteration 47, loss = 0.69197871\n",
      "Iteration 48, loss = 0.69186520\n",
      "Iteration 49, loss = 0.69175063\n",
      "Iteration 50, loss = 0.69163659\n",
      "Iteration 51, loss = 0.69151504\n",
      "Iteration 52, loss = 0.69140108\n",
      "Iteration 53, loss = 0.69132563\n",
      "Iteration 54, loss = 0.69123463\n",
      "Iteration 55, loss = 0.69113289\n",
      "Iteration 56, loss = 0.69104898\n",
      "Iteration 57, loss = 0.69099749\n",
      "Iteration 58, loss = 0.69086599\n",
      "Iteration 59, loss = 0.69080196\n",
      "Iteration 60, loss = 0.69073414\n",
      "Iteration 61, loss = 0.69068825\n",
      "Iteration 62, loss = 0.69059957\n",
      "Iteration 63, loss = 0.69054698\n",
      "Iteration 64, loss = 0.69047103\n",
      "Iteration 65, loss = 0.69040815\n",
      "Iteration 66, loss = 0.69036315\n",
      "Iteration 67, loss = 0.69029070\n",
      "Iteration 68, loss = 0.69024840\n",
      "Iteration 69, loss = 0.69017657\n",
      "Iteration 70, loss = 0.69012631\n",
      "Iteration 71, loss = 0.69007220\n",
      "Iteration 72, loss = 0.69002600\n",
      "Iteration 73, loss = 0.68998560\n",
      "Iteration 74, loss = 0.68991832\n",
      "Iteration 75, loss = 0.68986302\n",
      "Iteration 76, loss = 0.68981312\n",
      "Iteration 77, loss = 0.68976851\n",
      "Iteration 78, loss = 0.68972699\n",
      "Iteration 79, loss = 0.68967608\n",
      "Iteration 80, loss = 0.68961229\n",
      "Iteration 81, loss = 0.68957637\n",
      "Iteration 82, loss = 0.68952206\n",
      "Iteration 83, loss = 0.68946881\n",
      "Iteration 84, loss = 0.68941553\n",
      "Iteration 85, loss = 0.68936887\n",
      "Iteration 86, loss = 0.68931499\n",
      "Iteration 87, loss = 0.68926816\n",
      "Iteration 88, loss = 0.68922536\n",
      "Iteration 89, loss = 0.68916237\n",
      "Iteration 90, loss = 0.68911540\n",
      "Iteration 91, loss = 0.68906819\n",
      "Iteration 92, loss = 0.68900928\n",
      "Iteration 93, loss = 0.68895658\n",
      "Iteration 94, loss = 0.68890313\n",
      "Iteration 95, loss = 0.68885205\n",
      "Iteration 96, loss = 0.68879776\n",
      "Iteration 97, loss = 0.68874126\n",
      "Iteration 98, loss = 0.68868781\n",
      "Iteration 99, loss = 0.68863169\n",
      "Iteration 100, loss = 0.68858135\n",
      "Iteration 101, loss = 0.68851587\n",
      "Iteration 102, loss = 0.68845774\n",
      "Iteration 103, loss = 0.68840209\n",
      "Iteration 104, loss = 0.68834059\n",
      "Iteration 105, loss = 0.68828252\n",
      "Iteration 106, loss = 0.68822372\n",
      "Iteration 107, loss = 0.68816965\n",
      "Iteration 108, loss = 0.68809847\n",
      "Iteration 109, loss = 0.68803520\n",
      "Iteration 110, loss = 0.68797834\n",
      "Iteration 111, loss = 0.68790963\n",
      "Iteration 112, loss = 0.68785753\n",
      "Iteration 113, loss = 0.68778274\n",
      "Iteration 114, loss = 0.68771930\n",
      "Iteration 115, loss = 0.68765124\n",
      "Iteration 116, loss = 0.68758226\n",
      "Iteration 117, loss = 0.68751717\n",
      "Iteration 118, loss = 0.68744942\n",
      "Iteration 119, loss = 0.68737707\n",
      "Iteration 120, loss = 0.68731050\n",
      "Iteration 121, loss = 0.68723354\n",
      "Iteration 122, loss = 0.68716955\n",
      "Iteration 123, loss = 0.68708978\n",
      "Iteration 124, loss = 0.68701645\n",
      "Iteration 125, loss = 0.68694099\n",
      "Iteration 126, loss = 0.68686372\n",
      "Iteration 127, loss = 0.68678793\n",
      "Iteration 128, loss = 0.68671122\n",
      "Iteration 129, loss = 0.68663537\n",
      "Iteration 130, loss = 0.68655750\n",
      "Iteration 131, loss = 0.68647084\n",
      "Iteration 132, loss = 0.68639184\n",
      "Iteration 133, loss = 0.68630828\n",
      "Iteration 134, loss = 0.68623492\n",
      "Iteration 135, loss = 0.68614229\n",
      "Iteration 136, loss = 0.68605497\n",
      "Iteration 137, loss = 0.68596907\n",
      "Iteration 138, loss = 0.68589398\n",
      "Iteration 139, loss = 0.68579717\n",
      "Iteration 140, loss = 0.68570350\n",
      "Iteration 141, loss = 0.68561194\n",
      "Iteration 142, loss = 0.68552212\n",
      "Iteration 143, loss = 0.68542595\n",
      "Iteration 144, loss = 0.68533624\n",
      "Iteration 145, loss = 0.68524172\n",
      "Iteration 146, loss = 0.68514144\n",
      "Iteration 147, loss = 0.68504698\n",
      "Iteration 148, loss = 0.68494553\n",
      "Iteration 149, loss = 0.68484793\n",
      "Iteration 150, loss = 0.68475432\n",
      "Iteration 151, loss = 0.68464421\n",
      "Iteration 152, loss = 0.68454015\n",
      "Iteration 153, loss = 0.68443966\n",
      "Iteration 154, loss = 0.68432882\n",
      "Iteration 155, loss = 0.68422402\n",
      "Iteration 156, loss = 0.68411012\n",
      "Iteration 157, loss = 0.68400206\n",
      "Iteration 158, loss = 0.68388892\n",
      "Iteration 159, loss = 0.68377855\n",
      "Iteration 160, loss = 0.68367967\n",
      "Iteration 161, loss = 0.68354613\n",
      "Iteration 162, loss = 0.68343121\n",
      "Iteration 163, loss = 0.68331132\n",
      "Iteration 164, loss = 0.68319236\n",
      "Iteration 165, loss = 0.68307340\n",
      "Iteration 166, loss = 0.68294955\n",
      "Iteration 167, loss = 0.68282363\n",
      "Iteration 168, loss = 0.68269389\n",
      "Iteration 169, loss = 0.68256798\n",
      "Iteration 170, loss = 0.68244267\n",
      "Iteration 171, loss = 0.68232144\n",
      "Iteration 172, loss = 0.68217320\n",
      "Iteration 173, loss = 0.68203766\n",
      "Iteration 174, loss = 0.68190444\n",
      "Iteration 175, loss = 0.68176577\n",
      "Iteration 176, loss = 0.68162700\n",
      "Iteration 177, loss = 0.68148458\n",
      "Iteration 178, loss = 0.68133946\n",
      "Iteration 179, loss = 0.68119721\n",
      "Iteration 180, loss = 0.68106507\n",
      "Iteration 181, loss = 0.68091317\n",
      "Iteration 182, loss = 0.68076034\n",
      "Iteration 183, loss = 0.68061491\n",
      "Iteration 184, loss = 0.68045338\n",
      "Iteration 185, loss = 0.68029269\n",
      "Iteration 186, loss = 0.68013759\n",
      "Iteration 187, loss = 0.67997777\n",
      "Iteration 188, loss = 0.67981931\n",
      "Iteration 189, loss = 0.67965112\n",
      "Iteration 190, loss = 0.67948846\n",
      "Iteration 191, loss = 0.67932290\n",
      "Iteration 192, loss = 0.67916125\n",
      "Iteration 193, loss = 0.67898905\n",
      "Iteration 194, loss = 0.67881151\n",
      "Iteration 195, loss = 0.67863902\n",
      "Iteration 196, loss = 0.67845678\n",
      "Iteration 197, loss = 0.67828469\n",
      "Iteration 198, loss = 0.67810411\n",
      "Iteration 199, loss = 0.67792141\n",
      "Iteration 200, loss = 0.67772676\n",
      "Iteration 201, loss = 0.67754955\n",
      "Iteration 202, loss = 0.67736050\n",
      "Iteration 203, loss = 0.67716044\n",
      "Iteration 204, loss = 0.67698061\n",
      "Iteration 205, loss = 0.67677158\n",
      "Iteration 206, loss = 0.67657901\n",
      "Iteration 207, loss = 0.67637202\n",
      "Iteration 208, loss = 0.67617207\n",
      "Iteration 209, loss = 0.67596673\n",
      "Iteration 210, loss = 0.67575054\n",
      "Iteration 211, loss = 0.67554420\n",
      "Iteration 212, loss = 0.67533398\n",
      "Iteration 213, loss = 0.67511750\n",
      "Iteration 214, loss = 0.67489723\n",
      "Iteration 215, loss = 0.67468166\n",
      "Iteration 216, loss = 0.67445064\n",
      "Iteration 217, loss = 0.67423657\n",
      "Iteration 218, loss = 0.67399863\n",
      "Iteration 219, loss = 0.67376785\n",
      "Iteration 220, loss = 0.67353052\n",
      "Iteration 221, loss = 0.67329822\n",
      "Iteration 222, loss = 0.67307207\n",
      "Iteration 223, loss = 0.67282244\n",
      "Iteration 224, loss = 0.67257559\n",
      "Iteration 225, loss = 0.67231974\n",
      "Iteration 226, loss = 0.67207754\n",
      "Iteration 227, loss = 0.67183115\n",
      "Iteration 228, loss = 0.67156232\n",
      "Iteration 229, loss = 0.67129706\n",
      "Iteration 230, loss = 0.67103831\n",
      "Iteration 231, loss = 0.67077064\n",
      "Iteration 232, loss = 0.67050652\n",
      "Iteration 233, loss = 0.67022728\n",
      "Iteration 234, loss = 0.66997595\n",
      "Iteration 235, loss = 0.66967569\n",
      "Iteration 236, loss = 0.66938985\n",
      "Iteration 237, loss = 0.66910568\n",
      "Iteration 238, loss = 0.66882132\n",
      "Iteration 239, loss = 0.66853694\n",
      "Iteration 240, loss = 0.66823248\n",
      "Iteration 241, loss = 0.66793032\n",
      "Iteration 242, loss = 0.66762744\n",
      "Iteration 243, loss = 0.66733788\n",
      "Iteration 244, loss = 0.66701611\n",
      "Iteration 245, loss = 0.66670019\n",
      "Iteration 246, loss = 0.66638881\n",
      "Iteration 247, loss = 0.66606363\n",
      "Iteration 248, loss = 0.66574575\n",
      "Iteration 249, loss = 0.66541996\n",
      "Iteration 250, loss = 0.66509055\n",
      "Iteration 251, loss = 0.66475614\n",
      "Iteration 252, loss = 0.66445559\n",
      "Iteration 253, loss = 0.66409049\n",
      "Iteration 254, loss = 0.66372252\n",
      "Iteration 255, loss = 0.66337729\n",
      "Iteration 256, loss = 0.66304825\n",
      "Iteration 257, loss = 0.66266823\n",
      "Iteration 258, loss = 0.66230946\n",
      "Iteration 259, loss = 0.66194618\n",
      "Iteration 260, loss = 0.66156952\n",
      "Iteration 261, loss = 0.66120810\n",
      "Iteration 262, loss = 0.66082890\n",
      "Iteration 263, loss = 0.66046406\n",
      "Iteration 264, loss = 0.66005598\n",
      "Iteration 265, loss = 0.65966901\n",
      "Iteration 266, loss = 0.65927292\n",
      "Iteration 267, loss = 0.65888580\n",
      "Iteration 268, loss = 0.65847060\n",
      "Iteration 269, loss = 0.65807606\n",
      "Iteration 270, loss = 0.65765971\n",
      "Iteration 271, loss = 0.65723523\n",
      "Iteration 272, loss = 0.65681267\n",
      "Iteration 273, loss = 0.65639695\n",
      "Iteration 274, loss = 0.65596221\n",
      "Iteration 275, loss = 0.65553381\n",
      "Iteration 276, loss = 0.65507950\n",
      "Iteration 277, loss = 0.65464973\n",
      "Iteration 278, loss = 0.65419008\n",
      "Iteration 279, loss = 0.65374327\n",
      "Iteration 280, loss = 0.65328889\n",
      "Iteration 281, loss = 0.65281710\n",
      "Iteration 282, loss = 0.65234945\n",
      "Iteration 283, loss = 0.65189407\n",
      "Iteration 284, loss = 0.65141548\n",
      "Iteration 285, loss = 0.65091666\n",
      "Iteration 286, loss = 0.65046278\n",
      "Iteration 287, loss = 0.64995422\n",
      "Iteration 288, loss = 0.64944632\n",
      "Iteration 289, loss = 0.64894857\n",
      "Iteration 290, loss = 0.64843778\n",
      "Iteration 291, loss = 0.64793176\n",
      "Iteration 292, loss = 0.64740566\n",
      "Iteration 293, loss = 0.64688526\n",
      "Iteration 294, loss = 0.64640511\n",
      "Iteration 295, loss = 0.64582738\n",
      "Iteration 296, loss = 0.64530771\n",
      "Iteration 297, loss = 0.64474568\n",
      "Iteration 298, loss = 0.64421597\n",
      "Iteration 299, loss = 0.64366144\n",
      "Iteration 300, loss = 0.64310698\n",
      "Iteration 301, loss = 0.64253499\n",
      "Iteration 302, loss = 0.64197064\n",
      "Iteration 303, loss = 0.64139820\n",
      "Iteration 304, loss = 0.64082957\n",
      "Iteration 305, loss = 0.64024981\n",
      "Iteration 306, loss = 0.63963753\n",
      "Iteration 307, loss = 0.63904578\n",
      "Iteration 308, loss = 0.63846451\n",
      "Iteration 309, loss = 0.63784579\n",
      "Iteration 310, loss = 0.63722108\n",
      "Iteration 311, loss = 0.63662551\n",
      "Iteration 312, loss = 0.63598142\n",
      "Iteration 313, loss = 0.63535535\n",
      "Iteration 314, loss = 0.63473011\n",
      "Iteration 315, loss = 0.63408980\n",
      "Iteration 316, loss = 0.63343934\n",
      "Iteration 317, loss = 0.63278343\n",
      "Iteration 318, loss = 0.63213502\n",
      "Iteration 319, loss = 0.63146494\n",
      "Iteration 320, loss = 0.63079794\n",
      "Iteration 321, loss = 0.63010432\n",
      "Iteration 322, loss = 0.62945112\n",
      "Iteration 323, loss = 0.62873608\n",
      "Iteration 324, loss = 0.62804032\n",
      "Iteration 325, loss = 0.62735395\n",
      "Iteration 326, loss = 0.62665521\n",
      "Iteration 327, loss = 0.62597564\n",
      "Iteration 328, loss = 0.62522164\n",
      "Iteration 329, loss = 0.62452206\n",
      "Iteration 330, loss = 0.62377790\n",
      "Iteration 331, loss = 0.62307127\n",
      "Iteration 332, loss = 0.62231262\n",
      "Iteration 333, loss = 0.62155419\n",
      "Iteration 334, loss = 0.62081887\n",
      "Iteration 335, loss = 0.62007834\n",
      "Iteration 336, loss = 0.61930923\n",
      "Iteration 337, loss = 0.61853657\n",
      "Iteration 338, loss = 0.61777376\n",
      "Iteration 339, loss = 0.61704490\n",
      "Iteration 340, loss = 0.61623086\n",
      "Iteration 341, loss = 0.61542581\n",
      "Iteration 342, loss = 0.61464032\n",
      "Iteration 343, loss = 0.61385325\n",
      "Iteration 344, loss = 0.61303431\n",
      "Iteration 345, loss = 0.61222656\n",
      "Iteration 346, loss = 0.61146161\n",
      "Iteration 347, loss = 0.61063831\n",
      "Iteration 348, loss = 0.60977794\n",
      "Iteration 349, loss = 0.60895392\n",
      "Iteration 350, loss = 0.60814183\n",
      "Iteration 351, loss = 0.60726262\n",
      "Iteration 352, loss = 0.60644887\n",
      "Iteration 353, loss = 0.60559411\n",
      "Iteration 354, loss = 0.60473790\n",
      "Iteration 355, loss = 0.60399016\n",
      "Iteration 356, loss = 0.60300805\n",
      "Iteration 357, loss = 0.60215513\n",
      "Iteration 358, loss = 0.60128568\n",
      "Iteration 359, loss = 0.60040161\n",
      "Iteration 360, loss = 0.59952124\n",
      "Iteration 361, loss = 0.59866407\n",
      "Iteration 362, loss = 0.59774550\n",
      "Iteration 363, loss = 0.59685723\n",
      "Iteration 364, loss = 0.59596771\n",
      "Iteration 365, loss = 0.59507821\n",
      "Iteration 366, loss = 0.59416044\n",
      "Iteration 367, loss = 0.59325024\n",
      "Iteration 368, loss = 0.59232691\n",
      "Iteration 369, loss = 0.59141299\n",
      "Iteration 370, loss = 0.59046992\n",
      "Iteration 371, loss = 0.58953635\n",
      "Iteration 372, loss = 0.58861800\n",
      "Iteration 373, loss = 0.58767540\n",
      "Iteration 374, loss = 0.58673052\n",
      "Iteration 375, loss = 0.58578672\n",
      "Iteration 376, loss = 0.58483747\n",
      "Iteration 377, loss = 0.58387294\n",
      "Iteration 378, loss = 0.58289676\n",
      "Iteration 379, loss = 0.58195648\n",
      "Iteration 380, loss = 0.58096382\n",
      "Iteration 381, loss = 0.57999175\n",
      "Iteration 382, loss = 0.57902928\n",
      "Iteration 383, loss = 0.57801563\n",
      "Iteration 384, loss = 0.57704172\n",
      "Iteration 385, loss = 0.57606936\n",
      "Iteration 386, loss = 0.57507489\n",
      "Iteration 387, loss = 0.57405583\n",
      "Iteration 388, loss = 0.57304480\n",
      "Iteration 389, loss = 0.57203107\n",
      "Iteration 390, loss = 0.57107189\n",
      "Iteration 391, loss = 0.57000104\n",
      "Iteration 392, loss = 0.56900819\n",
      "Iteration 393, loss = 0.56795233\n",
      "Iteration 394, loss = 0.56691337\n",
      "Iteration 395, loss = 0.56590498\n",
      "Iteration 396, loss = 0.56486128\n",
      "Iteration 397, loss = 0.56383230\n",
      "Iteration 398, loss = 0.56277407\n",
      "Iteration 399, loss = 0.56171706\n",
      "Iteration 400, loss = 0.56064600\n",
      "Iteration 401, loss = 0.55959873\n",
      "Iteration 402, loss = 0.55855668\n",
      "Iteration 403, loss = 0.55743963\n",
      "Iteration 404, loss = 0.55635729\n",
      "Iteration 405, loss = 0.55525593\n",
      "Iteration 406, loss = 0.55425472\n",
      "Iteration 407, loss = 0.55307250\n",
      "Iteration 408, loss = 0.55198712\n",
      "Iteration 409, loss = 0.55084325\n",
      "Iteration 410, loss = 0.54972863\n",
      "Iteration 411, loss = 0.54860053\n",
      "Iteration 412, loss = 0.54749880\n",
      "Iteration 413, loss = 0.54630559\n",
      "Iteration 414, loss = 0.54516910\n",
      "Iteration 415, loss = 0.54400550\n",
      "Iteration 416, loss = 0.54285603\n",
      "Iteration 417, loss = 0.54170668\n",
      "Iteration 418, loss = 0.54053709\n",
      "Iteration 419, loss = 0.53936678\n",
      "Iteration 420, loss = 0.53818880\n",
      "Iteration 421, loss = 0.53702883\n",
      "Iteration 422, loss = 0.53589961\n",
      "Iteration 423, loss = 0.53470249\n",
      "Iteration 424, loss = 0.53356063\n",
      "Iteration 425, loss = 0.53240181\n",
      "Iteration 426, loss = 0.53128984\n",
      "Iteration 427, loss = 0.53009743\n",
      "Iteration 428, loss = 0.52893278\n",
      "Iteration 429, loss = 0.52779339\n",
      "Iteration 430, loss = 0.52665903\n",
      "Iteration 431, loss = 0.52545872\n",
      "Iteration 432, loss = 0.52430617\n",
      "Iteration 433, loss = 0.52314350\n",
      "Iteration 434, loss = 0.52204224\n",
      "Iteration 435, loss = 0.52085617\n",
      "Iteration 436, loss = 0.51968400\n",
      "Iteration 437, loss = 0.51850468\n",
      "Iteration 438, loss = 0.51736558\n",
      "Iteration 439, loss = 0.51623131\n",
      "Iteration 440, loss = 0.51506388\n",
      "Iteration 441, loss = 0.51389990\n",
      "Iteration 442, loss = 0.51271530\n",
      "Iteration 443, loss = 0.51155001\n",
      "Iteration 444, loss = 0.51040151\n",
      "Iteration 445, loss = 0.50922473\n",
      "Iteration 446, loss = 0.50807176\n",
      "Iteration 447, loss = 0.50692140\n",
      "Iteration 448, loss = 0.50573563\n",
      "Iteration 449, loss = 0.50457501\n",
      "Iteration 450, loss = 0.50343089\n",
      "Iteration 451, loss = 0.50225617\n",
      "Iteration 452, loss = 0.50111890\n",
      "Iteration 453, loss = 0.49995459\n",
      "Iteration 454, loss = 0.49880820\n",
      "Iteration 455, loss = 0.49766442\n",
      "Iteration 456, loss = 0.49646856\n",
      "Iteration 457, loss = 0.49530738\n",
      "Iteration 458, loss = 0.49419749\n",
      "Iteration 459, loss = 0.49300746\n",
      "Iteration 460, loss = 0.49187018\n",
      "Iteration 461, loss = 0.49074801\n",
      "Iteration 462, loss = 0.48955154\n",
      "Iteration 463, loss = 0.48841458\n",
      "Iteration 464, loss = 0.48726152\n",
      "Iteration 465, loss = 0.48610937\n",
      "Iteration 466, loss = 0.48495382\n",
      "Iteration 467, loss = 0.48389302\n",
      "Iteration 468, loss = 0.48268300\n",
      "Iteration 469, loss = 0.48154385\n",
      "Iteration 470, loss = 0.48038865\n",
      "Iteration 471, loss = 0.47925156\n",
      "Iteration 472, loss = 0.47812098\n",
      "Iteration 473, loss = 0.47700776\n",
      "Iteration 474, loss = 0.47581561\n",
      "Iteration 475, loss = 0.47467853\n",
      "Iteration 476, loss = 0.47354713\n",
      "Iteration 477, loss = 0.47253259\n",
      "Iteration 478, loss = 0.47130662\n",
      "Iteration 479, loss = 0.47015579\n",
      "Iteration 480, loss = 0.46904822\n",
      "Iteration 481, loss = 0.46790535\n",
      "Iteration 482, loss = 0.46677356\n",
      "Iteration 483, loss = 0.46566690\n",
      "Iteration 484, loss = 0.46455363\n",
      "Iteration 485, loss = 0.46340839\n",
      "Iteration 486, loss = 0.46231344\n",
      "Iteration 487, loss = 0.46139593\n",
      "Iteration 488, loss = 0.46005542\n",
      "Iteration 489, loss = 0.45901878\n",
      "Iteration 490, loss = 0.45786744\n",
      "Iteration 491, loss = 0.45676335\n",
      "Iteration 492, loss = 0.45566580\n",
      "Iteration 493, loss = 0.45457600\n",
      "Iteration 494, loss = 0.45345921\n",
      "Iteration 495, loss = 0.45238929\n",
      "Iteration 496, loss = 0.45126640\n",
      "Iteration 497, loss = 0.45017243\n",
      "Iteration 498, loss = 0.44910466\n",
      "Iteration 499, loss = 0.44799088\n",
      "Iteration 500, loss = 0.44694329\n",
      "Iteration 501, loss = 0.44581788\n",
      "Iteration 502, loss = 0.44474033\n",
      "Iteration 503, loss = 0.44370964\n",
      "Iteration 504, loss = 0.44258801\n",
      "Iteration 505, loss = 0.44155068\n",
      "Iteration 506, loss = 0.44047509\n",
      "Iteration 507, loss = 0.43941259\n",
      "Iteration 508, loss = 0.43833558\n",
      "Iteration 509, loss = 0.43727314\n",
      "Iteration 510, loss = 0.43619183\n",
      "Iteration 511, loss = 0.43515346\n",
      "Iteration 512, loss = 0.43408129\n",
      "Iteration 513, loss = 0.43304968\n",
      "Iteration 514, loss = 0.43199004\n",
      "Iteration 515, loss = 0.43097756\n",
      "Iteration 516, loss = 0.42990311\n",
      "Iteration 517, loss = 0.42888041\n",
      "Iteration 518, loss = 0.42782825\n",
      "Iteration 519, loss = 0.42680227\n",
      "Iteration 520, loss = 0.42576623\n",
      "Iteration 521, loss = 0.42474741\n",
      "Iteration 522, loss = 0.42370222\n",
      "Iteration 523, loss = 0.42268342\n",
      "Iteration 524, loss = 0.42165961\n",
      "Iteration 525, loss = 0.42065247\n",
      "Iteration 526, loss = 0.41961569\n",
      "Iteration 527, loss = 0.41868353\n",
      "Iteration 528, loss = 0.41761304\n",
      "Iteration 529, loss = 0.41665226\n",
      "Iteration 530, loss = 0.41562994\n",
      "Iteration 531, loss = 0.41459952\n",
      "Iteration 532, loss = 0.41363475\n",
      "Iteration 533, loss = 0.41262842\n",
      "Iteration 534, loss = 0.41163652\n",
      "Iteration 535, loss = 0.41063722\n",
      "Iteration 536, loss = 0.40965817\n",
      "Iteration 537, loss = 0.40874758\n",
      "Iteration 538, loss = 0.40775349\n",
      "Iteration 539, loss = 0.40677756\n",
      "Iteration 540, loss = 0.40579331\n",
      "Iteration 541, loss = 0.40486209\n",
      "Iteration 542, loss = 0.40386646\n",
      "Iteration 543, loss = 0.40291417\n",
      "Iteration 544, loss = 0.40196595\n",
      "Iteration 545, loss = 0.40097500\n",
      "Iteration 546, loss = 0.40007564\n",
      "Iteration 547, loss = 0.39908963\n",
      "Iteration 548, loss = 0.39817957\n",
      "Iteration 549, loss = 0.39721709\n",
      "Iteration 550, loss = 0.39627380\n",
      "Iteration 551, loss = 0.39537311\n",
      "Iteration 552, loss = 0.39441943\n",
      "Iteration 553, loss = 0.39350128\n",
      "Iteration 554, loss = 0.39258745\n",
      "Iteration 555, loss = 0.39171150\n",
      "Iteration 556, loss = 0.39075576\n",
      "Iteration 557, loss = 0.38982878\n",
      "Iteration 558, loss = 0.38891497\n",
      "Iteration 559, loss = 0.38802238\n",
      "Iteration 560, loss = 0.38712104\n",
      "Iteration 561, loss = 0.38621732\n",
      "Iteration 562, loss = 0.38533324\n",
      "Iteration 563, loss = 0.38444149\n",
      "Iteration 564, loss = 0.38354518\n",
      "Iteration 565, loss = 0.38265102\n",
      "Iteration 566, loss = 0.38180867\n",
      "Iteration 567, loss = 0.38088868\n",
      "Iteration 568, loss = 0.38007200\n",
      "Iteration 569, loss = 0.37918245\n",
      "Iteration 570, loss = 0.37831827\n",
      "Iteration 571, loss = 0.37740955\n",
      "Iteration 572, loss = 0.37660516\n",
      "Iteration 573, loss = 0.37570457\n",
      "Iteration 574, loss = 0.37485430\n",
      "Iteration 575, loss = 0.37400181\n",
      "Iteration 576, loss = 0.37315212\n",
      "Iteration 577, loss = 0.37229729\n",
      "Iteration 578, loss = 0.37145665\n",
      "Iteration 579, loss = 0.37062473\n",
      "Iteration 580, loss = 0.36979864\n",
      "Iteration 581, loss = 0.36901511\n",
      "Iteration 582, loss = 0.36818945\n",
      "Iteration 583, loss = 0.36729591\n",
      "Iteration 584, loss = 0.36647671\n",
      "Iteration 585, loss = 0.36565672\n",
      "Iteration 586, loss = 0.36486474\n",
      "Iteration 587, loss = 0.36407118\n",
      "Iteration 588, loss = 0.36323229\n",
      "Iteration 589, loss = 0.36244376\n",
      "Iteration 590, loss = 0.36160297\n",
      "Iteration 591, loss = 0.36080695\n",
      "Iteration 592, loss = 0.36004031\n",
      "Iteration 593, loss = 0.35928651\n",
      "Iteration 594, loss = 0.35843394\n",
      "Iteration 595, loss = 0.35765975\n",
      "Iteration 596, loss = 0.35687409\n",
      "Iteration 597, loss = 0.35607803\n",
      "Iteration 598, loss = 0.35530706\n",
      "Iteration 599, loss = 0.35465339\n",
      "Iteration 600, loss = 0.35376855\n",
      "Iteration 601, loss = 0.35299682\n",
      "Iteration 602, loss = 0.35224962\n",
      "Iteration 603, loss = 0.35147231\n",
      "Iteration 604, loss = 0.35071367\n",
      "Iteration 605, loss = 0.34999564\n",
      "Iteration 606, loss = 0.34918746\n",
      "Iteration 607, loss = 0.34843144\n",
      "Iteration 608, loss = 0.34773372\n",
      "Iteration 609, loss = 0.34698490\n",
      "Iteration 610, loss = 0.34623257\n",
      "Iteration 611, loss = 0.34552621\n",
      "Iteration 612, loss = 0.34478248\n",
      "Iteration 613, loss = 0.34407468\n",
      "Iteration 614, loss = 0.34331758\n",
      "Iteration 615, loss = 0.34258073\n",
      "Iteration 616, loss = 0.34185755\n",
      "Iteration 617, loss = 0.34119904\n",
      "Iteration 618, loss = 0.34042776\n",
      "Iteration 619, loss = 0.33973407\n",
      "Iteration 620, loss = 0.33898043\n",
      "Iteration 621, loss = 0.33828620\n",
      "Iteration 622, loss = 0.33757624\n",
      "Iteration 623, loss = 0.33688483\n",
      "Iteration 624, loss = 0.33617245\n",
      "Iteration 625, loss = 0.33547806\n",
      "Iteration 626, loss = 0.33481541\n",
      "Iteration 627, loss = 0.33410064\n",
      "Iteration 628, loss = 0.33341609\n",
      "Iteration 629, loss = 0.33273179\n",
      "Iteration 630, loss = 0.33205353\n",
      "Iteration 631, loss = 0.33137642\n",
      "Iteration 632, loss = 0.33074552\n",
      "Iteration 633, loss = 0.33008409\n",
      "Iteration 634, loss = 0.32935693\n",
      "Iteration 635, loss = 0.32870276\n",
      "Iteration 636, loss = 0.32801774\n",
      "Iteration 637, loss = 0.32735962\n",
      "Iteration 638, loss = 0.32674889\n",
      "Iteration 639, loss = 0.32607397\n",
      "Iteration 640, loss = 0.32541220\n",
      "Iteration 641, loss = 0.32475530\n",
      "Iteration 642, loss = 0.32414641\n",
      "Iteration 643, loss = 0.32344950\n",
      "Iteration 644, loss = 0.32292972\n",
      "Iteration 645, loss = 0.32220041\n",
      "Iteration 646, loss = 0.32153511\n",
      "Iteration 647, loss = 0.32091476\n",
      "Iteration 648, loss = 0.32028766\n",
      "Iteration 649, loss = 0.31965996\n",
      "Iteration 650, loss = 0.31904468\n",
      "Iteration 651, loss = 0.31840301\n",
      "Iteration 652, loss = 0.31781365\n",
      "Iteration 653, loss = 0.31717021\n",
      "Iteration 654, loss = 0.31659034\n",
      "Iteration 655, loss = 0.31594530\n",
      "Iteration 656, loss = 0.31535867\n",
      "Iteration 657, loss = 0.31471954\n",
      "Iteration 658, loss = 0.31418605\n",
      "Iteration 659, loss = 0.31355835\n",
      "Iteration 660, loss = 0.31291844\n",
      "Iteration 661, loss = 0.31238143\n",
      "Iteration 662, loss = 0.31173732\n",
      "Iteration 663, loss = 0.31115922\n",
      "Iteration 664, loss = 0.31058206\n",
      "Iteration 665, loss = 0.30996732\n",
      "Iteration 666, loss = 0.30943663\n",
      "Iteration 667, loss = 0.30879600\n",
      "Iteration 668, loss = 0.30825052\n",
      "Iteration 669, loss = 0.30770917\n",
      "Iteration 670, loss = 0.30707289\n",
      "Iteration 671, loss = 0.30649571\n",
      "Iteration 672, loss = 0.30592985\n",
      "Iteration 673, loss = 0.30541459\n",
      "Iteration 674, loss = 0.30480114\n",
      "Iteration 675, loss = 0.30424749\n",
      "Iteration 676, loss = 0.30367186\n",
      "Iteration 677, loss = 0.30311992\n",
      "Iteration 678, loss = 0.30256597\n",
      "Iteration 679, loss = 0.30203950\n",
      "Iteration 680, loss = 0.30148868\n",
      "Iteration 681, loss = 0.30092023\n",
      "Iteration 682, loss = 0.30036906\n",
      "Iteration 683, loss = 0.29982563\n",
      "Iteration 684, loss = 0.29930642\n",
      "Iteration 685, loss = 0.29874636\n",
      "Iteration 686, loss = 0.29820290\n",
      "Iteration 687, loss = 0.29765674\n",
      "Iteration 688, loss = 0.29715154\n",
      "Iteration 689, loss = 0.29661975\n",
      "Iteration 690, loss = 0.29626074\n",
      "Iteration 691, loss = 0.29559031\n",
      "Iteration 692, loss = 0.29503146\n",
      "Iteration 693, loss = 0.29449408\n",
      "Iteration 694, loss = 0.29399135\n",
      "Iteration 695, loss = 0.29351217\n",
      "Iteration 696, loss = 0.29298070\n",
      "Iteration 697, loss = 0.29245411\n",
      "Iteration 698, loss = 0.29203600\n",
      "Iteration 699, loss = 0.29140413\n",
      "Iteration 700, loss = 0.29095926\n",
      "Iteration 701, loss = 0.29051929\n",
      "Iteration 702, loss = 0.28991554\n",
      "Iteration 703, loss = 0.28943170\n",
      "Iteration 704, loss = 0.28892504\n",
      "Iteration 705, loss = 0.28845099\n",
      "Iteration 706, loss = 0.28792397\n",
      "Iteration 707, loss = 0.28742674\n",
      "Iteration 708, loss = 0.28693726\n",
      "Iteration 709, loss = 0.28644710\n",
      "Iteration 710, loss = 0.28610326\n",
      "Iteration 711, loss = 0.28550139\n",
      "Iteration 712, loss = 0.28499620\n",
      "Iteration 713, loss = 0.28456268\n",
      "Iteration 714, loss = 0.28407748\n",
      "Iteration 715, loss = 0.28361128\n",
      "Iteration 716, loss = 0.28309093\n",
      "Iteration 717, loss = 0.28262581\n",
      "Iteration 718, loss = 0.28214881\n",
      "Iteration 719, loss = 0.28169459\n",
      "Iteration 720, loss = 0.28121830\n",
      "Iteration 721, loss = 0.28080058\n",
      "Iteration 722, loss = 0.28027790\n",
      "Iteration 723, loss = 0.27986240\n",
      "Iteration 724, loss = 0.27935876\n",
      "Iteration 725, loss = 0.27891474\n",
      "Iteration 726, loss = 0.27846178\n",
      "Iteration 727, loss = 0.27799960\n",
      "Iteration 728, loss = 0.27754948\n",
      "Iteration 729, loss = 0.27709873\n",
      "Iteration 730, loss = 0.27667559\n",
      "Iteration 731, loss = 0.27618541\n",
      "Iteration 732, loss = 0.27573929\n",
      "Iteration 733, loss = 0.27532515\n",
      "Iteration 734, loss = 0.27495355\n",
      "Iteration 735, loss = 0.27440793\n",
      "Iteration 736, loss = 0.27409558\n",
      "Iteration 737, loss = 0.27355132\n",
      "Iteration 738, loss = 0.27312896\n",
      "Iteration 739, loss = 0.27268857\n",
      "Iteration 740, loss = 0.27223738\n",
      "Iteration 741, loss = 0.27179962\n",
      "Iteration 742, loss = 0.27146545\n",
      "Iteration 743, loss = 0.27094372\n",
      "Iteration 744, loss = 0.27056521\n",
      "Iteration 745, loss = 0.27009459\n",
      "Iteration 746, loss = 0.26972919\n",
      "Iteration 747, loss = 0.26927387\n",
      "Iteration 748, loss = 0.26884165\n",
      "Iteration 749, loss = 0.26842765\n",
      "Iteration 750, loss = 0.26803266\n",
      "Iteration 751, loss = 0.26758067\n",
      "Iteration 752, loss = 0.26721246\n",
      "Iteration 753, loss = 0.26676546\n",
      "Iteration 754, loss = 0.26637725\n",
      "Iteration 755, loss = 0.26605034\n",
      "Iteration 756, loss = 0.26565075\n",
      "Iteration 757, loss = 0.26523238\n",
      "Iteration 758, loss = 0.26474409\n",
      "Iteration 759, loss = 0.26433894\n",
      "Iteration 760, loss = 0.26393361\n",
      "Iteration 761, loss = 0.26354540\n",
      "Iteration 762, loss = 0.26313043\n",
      "Iteration 763, loss = 0.26273541\n",
      "Iteration 764, loss = 0.26234776\n",
      "Iteration 765, loss = 0.26208920\n",
      "Iteration 766, loss = 0.26159819\n",
      "Iteration 767, loss = 0.26131481\n",
      "Iteration 768, loss = 0.26079814\n",
      "Iteration 769, loss = 0.26040497\n",
      "Iteration 770, loss = 0.26003884\n",
      "Iteration 771, loss = 0.25983896\n",
      "Iteration 772, loss = 0.25931103\n",
      "Iteration 773, loss = 0.25888884\n",
      "Iteration 774, loss = 0.25849631\n",
      "Iteration 775, loss = 0.25812882\n",
      "Iteration 776, loss = 0.25776899\n",
      "Iteration 777, loss = 0.25736070\n",
      "Iteration 778, loss = 0.25701109\n",
      "Iteration 779, loss = 0.25670537\n",
      "Iteration 780, loss = 0.25625257\n",
      "Iteration 781, loss = 0.25589492\n",
      "Iteration 782, loss = 0.25559584\n",
      "Iteration 783, loss = 0.25516092\n",
      "Iteration 784, loss = 0.25484057\n",
      "Iteration 785, loss = 0.25458480\n",
      "Iteration 786, loss = 0.25409902\n",
      "Iteration 787, loss = 0.25374744\n",
      "Iteration 788, loss = 0.25336062\n",
      "Iteration 789, loss = 0.25299910\n",
      "Iteration 790, loss = 0.25263909\n",
      "Iteration 791, loss = 0.25237989\n",
      "Iteration 792, loss = 0.25194539\n",
      "Iteration 793, loss = 0.25158844\n",
      "Iteration 794, loss = 0.25135135\n",
      "Iteration 795, loss = 0.25089497\n",
      "Iteration 796, loss = 0.25058344\n",
      "Iteration 797, loss = 0.25017771\n",
      "Iteration 798, loss = 0.24984892\n",
      "Iteration 799, loss = 0.24952253\n",
      "Iteration 800, loss = 0.24921859\n",
      "Iteration 801, loss = 0.24882755\n",
      "Iteration 802, loss = 0.24849004\n",
      "Iteration 803, loss = 0.24811768\n",
      "Iteration 804, loss = 0.24784570\n",
      "Iteration 805, loss = 0.24753259\n",
      "Iteration 806, loss = 0.24714609\n",
      "Iteration 807, loss = 0.24679482\n",
      "Iteration 808, loss = 0.24648000\n",
      "Iteration 809, loss = 0.24617236\n",
      "Iteration 810, loss = 0.24584206\n",
      "Iteration 811, loss = 0.24548930\n",
      "Iteration 812, loss = 0.24513477\n",
      "Iteration 813, loss = 0.24481048\n",
      "Iteration 814, loss = 0.24447980\n",
      "Iteration 815, loss = 0.24431445\n",
      "Iteration 816, loss = 0.24383841\n",
      "Iteration 817, loss = 0.24359996\n",
      "Iteration 818, loss = 0.24322361\n",
      "Iteration 819, loss = 0.24285913\n",
      "Iteration 820, loss = 0.24255883\n",
      "Iteration 821, loss = 0.24224053\n",
      "Iteration 822, loss = 0.24194899\n",
      "Iteration 823, loss = 0.24161099\n",
      "Iteration 824, loss = 0.24127702\n",
      "Iteration 825, loss = 0.24097346\n",
      "Iteration 826, loss = 0.24066351\n",
      "Iteration 827, loss = 0.24037528\n",
      "Iteration 828, loss = 0.24004282\n",
      "Iteration 829, loss = 0.23972163\n",
      "Iteration 830, loss = 0.23947713\n",
      "Iteration 831, loss = 0.23909187\n",
      "Iteration 832, loss = 0.23880023\n",
      "Iteration 833, loss = 0.23851149\n",
      "Iteration 834, loss = 0.23820932\n",
      "Iteration 835, loss = 0.23796532\n",
      "Iteration 836, loss = 0.23763532\n",
      "Iteration 837, loss = 0.23731774\n",
      "Iteration 838, loss = 0.23699417\n",
      "Iteration 839, loss = 0.23669216\n",
      "Iteration 840, loss = 0.23637793\n",
      "Iteration 841, loss = 0.23608225\n",
      "Iteration 842, loss = 0.23578384\n",
      "Iteration 843, loss = 0.23547661\n",
      "Iteration 844, loss = 0.23520056\n",
      "Iteration 845, loss = 0.23497918\n",
      "Iteration 846, loss = 0.23459899\n",
      "Iteration 847, loss = 0.23429453\n",
      "Iteration 848, loss = 0.23402711\n",
      "Iteration 849, loss = 0.23380279\n",
      "Iteration 850, loss = 0.23346309\n",
      "Iteration 851, loss = 0.23319329\n",
      "Iteration 852, loss = 0.23286806\n",
      "Iteration 853, loss = 0.23256386\n",
      "Iteration 854, loss = 0.23232343\n",
      "Iteration 855, loss = 0.23203725\n",
      "Iteration 856, loss = 0.23173372\n",
      "Iteration 857, loss = 0.23144279\n",
      "Iteration 858, loss = 0.23118800\n",
      "Iteration 859, loss = 0.23087897\n",
      "Iteration 860, loss = 0.23059547\n",
      "Iteration 861, loss = 0.23041738\n",
      "Iteration 862, loss = 0.23005582\n",
      "Iteration 863, loss = 0.22978874\n",
      "Iteration 864, loss = 0.22951349\n",
      "Iteration 865, loss = 0.22922742\n",
      "Iteration 866, loss = 0.22893987\n",
      "Iteration 867, loss = 0.22865716\n",
      "Iteration 868, loss = 0.22845053\n",
      "Iteration 869, loss = 0.22811355\n",
      "Iteration 870, loss = 0.22783553\n",
      "Iteration 871, loss = 0.22762156\n",
      "Iteration 872, loss = 0.22739395\n",
      "Iteration 873, loss = 0.22706716\n",
      "Iteration 874, loss = 0.22675969\n",
      "Iteration 875, loss = 0.22652569\n",
      "Iteration 876, loss = 0.22624242\n",
      "Iteration 877, loss = 0.22597047\n",
      "Iteration 878, loss = 0.22573616\n",
      "Iteration 879, loss = 0.22544881\n",
      "Iteration 880, loss = 0.22519580\n",
      "Iteration 881, loss = 0.22494689\n",
      "Iteration 882, loss = 0.22483808\n",
      "Iteration 883, loss = 0.22449466\n",
      "Iteration 884, loss = 0.22413743\n",
      "Iteration 885, loss = 0.22387295\n",
      "Iteration 886, loss = 0.22363009\n",
      "Iteration 887, loss = 0.22336458\n",
      "Iteration 888, loss = 0.22313863\n",
      "Iteration 889, loss = 0.22286309\n",
      "Iteration 890, loss = 0.22262427\n",
      "Iteration 891, loss = 0.22235970\n",
      "Iteration 892, loss = 0.22212182\n",
      "Iteration 893, loss = 0.22183964\n",
      "Iteration 894, loss = 0.22158679\n",
      "Iteration 895, loss = 0.22134681\n",
      "Iteration 896, loss = 0.22108870\n",
      "Iteration 897, loss = 0.22087949\n",
      "Iteration 898, loss = 0.22061259\n",
      "Iteration 899, loss = 0.22039734\n",
      "Iteration 900, loss = 0.22015096\n",
      "Iteration 901, loss = 0.21994224\n",
      "Iteration 902, loss = 0.21962104\n",
      "Iteration 903, loss = 0.21934788\n",
      "Iteration 904, loss = 0.21910333\n",
      "Iteration 905, loss = 0.21894706\n",
      "Iteration 906, loss = 0.21864713\n",
      "Iteration 907, loss = 0.21838087\n",
      "Iteration 908, loss = 0.21817037\n",
      "Iteration 909, loss = 0.21790381\n",
      "Iteration 910, loss = 0.21775408\n",
      "Iteration 911, loss = 0.21750882\n",
      "Iteration 912, loss = 0.21717275\n",
      "Iteration 913, loss = 0.21705660\n",
      "Iteration 914, loss = 0.21673190\n",
      "Iteration 915, loss = 0.21648747\n",
      "Iteration 916, loss = 0.21626506\n",
      "Iteration 917, loss = 0.21599243\n",
      "Iteration 918, loss = 0.21583211\n",
      "Iteration 919, loss = 0.21553526\n",
      "Iteration 920, loss = 0.21533051\n",
      "Iteration 921, loss = 0.21505790\n",
      "Iteration 922, loss = 0.21484039\n",
      "Iteration 923, loss = 0.21460868\n",
      "Iteration 924, loss = 0.21437184\n",
      "Iteration 925, loss = 0.21416162\n",
      "Iteration 926, loss = 0.21392661\n",
      "Iteration 927, loss = 0.21371382\n",
      "Iteration 928, loss = 0.21346496\n",
      "Iteration 929, loss = 0.21322057\n",
      "Iteration 930, loss = 0.21300838\n",
      "Iteration 931, loss = 0.21278412\n",
      "Iteration 932, loss = 0.21256251\n",
      "Iteration 933, loss = 0.21241177\n",
      "Iteration 934, loss = 0.21220166\n",
      "Iteration 935, loss = 0.21190117\n",
      "Iteration 936, loss = 0.21165608\n",
      "Iteration 937, loss = 0.21144175\n",
      "Iteration 938, loss = 0.21135485\n",
      "Iteration 939, loss = 0.21103376\n",
      "Iteration 940, loss = 0.21084744\n",
      "Iteration 941, loss = 0.21053212\n",
      "Iteration 942, loss = 0.21036681\n",
      "Iteration 943, loss = 0.21020003\n",
      "Iteration 944, loss = 0.20994448\n",
      "Iteration 945, loss = 0.20971687\n",
      "Iteration 946, loss = 0.20947119\n",
      "Iteration 947, loss = 0.20924018\n",
      "Iteration 948, loss = 0.20907059\n",
      "Iteration 949, loss = 0.20882416\n",
      "Iteration 950, loss = 0.20864683\n",
      "Iteration 951, loss = 0.20844133\n",
      "Iteration 952, loss = 0.20818977\n",
      "Iteration 953, loss = 0.20797509\n",
      "Iteration 954, loss = 0.20785722\n",
      "Iteration 955, loss = 0.20756664\n",
      "Iteration 956, loss = 0.20737977\n",
      "Iteration 957, loss = 0.20742084\n",
      "Iteration 958, loss = 0.20705209\n",
      "Iteration 959, loss = 0.20675770\n",
      "Iteration 960, loss = 0.20661312\n",
      "Iteration 961, loss = 0.20636673\n",
      "Iteration 962, loss = 0.20619820\n",
      "Iteration 963, loss = 0.20592395\n",
      "Iteration 964, loss = 0.20577411\n",
      "Iteration 965, loss = 0.20551042\n",
      "Iteration 966, loss = 0.20536169\n",
      "Iteration 967, loss = 0.20509625\n",
      "Iteration 968, loss = 0.20493037\n",
      "Iteration 969, loss = 0.20467181\n",
      "Iteration 970, loss = 0.20456703\n",
      "Iteration 971, loss = 0.20433891\n",
      "Iteration 972, loss = 0.20414404\n",
      "Iteration 973, loss = 0.20392604\n",
      "Iteration 974, loss = 0.20370220\n",
      "Iteration 975, loss = 0.20347361\n",
      "Iteration 976, loss = 0.20333414\n",
      "Iteration 977, loss = 0.20313686\n",
      "Iteration 978, loss = 0.20290238\n",
      "Iteration 979, loss = 0.20269174\n",
      "Iteration 980, loss = 0.20250468\n",
      "Iteration 981, loss = 0.20236894\n",
      "Iteration 982, loss = 0.20218712\n",
      "Iteration 983, loss = 0.20196591\n",
      "Iteration 984, loss = 0.20177342\n",
      "Iteration 985, loss = 0.20155314\n",
      "Iteration 986, loss = 0.20140797\n",
      "Iteration 987, loss = 0.20116479\n",
      "Iteration 988, loss = 0.20097564\n",
      "Iteration 989, loss = 0.20088242\n",
      "Iteration 990, loss = 0.20058194\n",
      "Iteration 991, loss = 0.20040338\n",
      "Iteration 992, loss = 0.20035720\n",
      "Iteration 993, loss = 0.20005933\n",
      "Iteration 994, loss = 0.19987890\n",
      "Iteration 995, loss = 0.19968845\n",
      "Iteration 996, loss = 0.19948021\n",
      "Iteration 997, loss = 0.19928763\n",
      "Iteration 998, loss = 0.19912473\n",
      "Iteration 999, loss = 0.19891454\n",
      "Iteration 1000, loss = 0.19872408\n",
      "Iteration 1001, loss = 0.19855835\n",
      "Iteration 1002, loss = 0.19841594\n",
      "Iteration 1003, loss = 0.19827822\n",
      "Iteration 1004, loss = 0.19809195\n",
      "Iteration 1005, loss = 0.19791672\n",
      "Iteration 1006, loss = 0.19766834\n",
      "Iteration 1007, loss = 0.19745468\n",
      "Iteration 1008, loss = 0.19729889\n",
      "Iteration 1009, loss = 0.19710078\n",
      "Iteration 1010, loss = 0.19693315\n",
      "Iteration 1011, loss = 0.19672129\n",
      "Iteration 1012, loss = 0.19656727\n",
      "Iteration 1013, loss = 0.19638999\n",
      "Iteration 1014, loss = 0.19625776\n",
      "Iteration 1015, loss = 0.19603646\n",
      "Iteration 1016, loss = 0.19586321\n",
      "Iteration 1017, loss = 0.19574845\n",
      "Iteration 1018, loss = 0.19551332\n",
      "Iteration 1019, loss = 0.19532103\n",
      "Iteration 1020, loss = 0.19519377\n",
      "Iteration 1021, loss = 0.19506995\n",
      "Iteration 1022, loss = 0.19480509\n",
      "Iteration 1023, loss = 0.19462674\n",
      "Iteration 1024, loss = 0.19444261\n",
      "Iteration 1025, loss = 0.19434903\n",
      "Iteration 1026, loss = 0.19410805\n",
      "Iteration 1027, loss = 0.19400837\n",
      "Iteration 1028, loss = 0.19375208\n",
      "Iteration 1029, loss = 0.19360073\n",
      "Iteration 1030, loss = 0.19341640\n",
      "Iteration 1031, loss = 0.19328860\n",
      "Iteration 1032, loss = 0.19308184\n",
      "Iteration 1033, loss = 0.19294905\n",
      "Iteration 1034, loss = 0.19272948\n",
      "Iteration 1035, loss = 0.19255609\n",
      "Iteration 1036, loss = 0.19242969\n",
      "Iteration 1037, loss = 0.19221820\n",
      "Iteration 1038, loss = 0.19220778\n",
      "Iteration 1039, loss = 0.19191637\n",
      "Iteration 1040, loss = 0.19171633\n",
      "Iteration 1041, loss = 0.19172457\n",
      "Iteration 1042, loss = 0.19146165\n",
      "Iteration 1043, loss = 0.19123538\n",
      "Iteration 1044, loss = 0.19110360\n",
      "Iteration 1045, loss = 0.19092237\n",
      "Iteration 1046, loss = 0.19071865\n",
      "Iteration 1047, loss = 0.19056150\n",
      "Iteration 1048, loss = 0.19038330\n",
      "Iteration 1049, loss = 0.19028642\n",
      "Iteration 1050, loss = 0.19006952\n",
      "Iteration 1051, loss = 0.18989502\n",
      "Iteration 1052, loss = 0.18973304\n",
      "Iteration 1053, loss = 0.18959164\n",
      "Iteration 1054, loss = 0.18942777\n",
      "Iteration 1055, loss = 0.18930112\n",
      "Iteration 1056, loss = 0.18910715\n",
      "Iteration 1057, loss = 0.18894988\n",
      "Iteration 1058, loss = 0.18880461\n",
      "Iteration 1059, loss = 0.18867539\n",
      "Iteration 1060, loss = 0.18849105\n",
      "Iteration 1061, loss = 0.18834448\n",
      "Iteration 1062, loss = 0.18816497\n",
      "Iteration 1063, loss = 0.18800138\n",
      "Iteration 1064, loss = 0.18780473\n",
      "Iteration 1065, loss = 0.18766045\n",
      "Iteration 1066, loss = 0.18749176\n",
      "Iteration 1067, loss = 0.18739325\n",
      "Iteration 1068, loss = 0.18722342\n",
      "Iteration 1069, loss = 0.18703400\n",
      "Iteration 1070, loss = 0.18689188\n",
      "Iteration 1071, loss = 0.18675699\n",
      "Iteration 1072, loss = 0.18654762\n",
      "Iteration 1073, loss = 0.18642844\n",
      "Iteration 1074, loss = 0.18628893\n",
      "Iteration 1075, loss = 0.18608467\n",
      "Iteration 1076, loss = 0.18593282\n",
      "Iteration 1077, loss = 0.18579804\n",
      "Iteration 1078, loss = 0.18562588\n",
      "Iteration 1079, loss = 0.18547154\n",
      "Iteration 1080, loss = 0.18536301\n",
      "Iteration 1081, loss = 0.18517121\n",
      "Iteration 1082, loss = 0.18499570\n",
      "Iteration 1083, loss = 0.18494129\n",
      "Iteration 1084, loss = 0.18471669\n",
      "Iteration 1085, loss = 0.18464697\n",
      "Iteration 1086, loss = 0.18440998\n",
      "Iteration 1087, loss = 0.18425943\n",
      "Iteration 1088, loss = 0.18410540\n",
      "Iteration 1089, loss = 0.18394645\n",
      "Iteration 1090, loss = 0.18379979\n",
      "Iteration 1091, loss = 0.18364378\n",
      "Iteration 1092, loss = 0.18350538\n",
      "Iteration 1093, loss = 0.18342092\n",
      "Iteration 1094, loss = 0.18324320\n",
      "Iteration 1095, loss = 0.18306940\n",
      "Iteration 1096, loss = 0.18299012\n",
      "Iteration 1097, loss = 0.18275430\n",
      "Iteration 1098, loss = 0.18258696\n",
      "Iteration 1099, loss = 0.18247831\n",
      "Iteration 1100, loss = 0.18229862\n",
      "Iteration 1101, loss = 0.18215514\n",
      "Iteration 1102, loss = 0.18206372\n",
      "Iteration 1103, loss = 0.18191513\n",
      "Iteration 1104, loss = 0.18172692\n",
      "Iteration 1105, loss = 0.18157486\n",
      "Iteration 1106, loss = 0.18144267\n",
      "Iteration 1107, loss = 0.18127798\n",
      "Iteration 1108, loss = 0.18116026\n",
      "Iteration 1109, loss = 0.18102825\n",
      "Iteration 1110, loss = 0.18098639\n",
      "Iteration 1111, loss = 0.18072606\n",
      "Iteration 1112, loss = 0.18058580\n",
      "Iteration 1113, loss = 0.18048399\n",
      "Iteration 1114, loss = 0.18029374\n",
      "Iteration 1115, loss = 0.18016689\n",
      "Iteration 1116, loss = 0.18010192\n",
      "Iteration 1117, loss = 0.17985867\n",
      "Iteration 1118, loss = 0.17975121\n",
      "Iteration 1119, loss = 0.17959245\n",
      "Iteration 1120, loss = 0.17952155\n",
      "Iteration 1121, loss = 0.17926231\n",
      "Iteration 1122, loss = 0.17932333\n",
      "Iteration 1123, loss = 0.17902446\n",
      "Iteration 1124, loss = 0.17890117\n",
      "Iteration 1125, loss = 0.17878538\n",
      "Iteration 1126, loss = 0.17861322\n",
      "Iteration 1127, loss = 0.17845710\n",
      "Iteration 1128, loss = 0.17848329\n",
      "Iteration 1129, loss = 0.17823560\n",
      "Iteration 1130, loss = 0.17811059\n",
      "Iteration 1131, loss = 0.17790324\n",
      "Iteration 1132, loss = 0.17775853\n",
      "Iteration 1133, loss = 0.17766874\n",
      "Iteration 1134, loss = 0.17755146\n",
      "Iteration 1135, loss = 0.17742988\n",
      "Iteration 1136, loss = 0.17729891\n",
      "Iteration 1137, loss = 0.17715376\n",
      "Iteration 1138, loss = 0.17693388\n",
      "Iteration 1139, loss = 0.17679939\n",
      "Iteration 1140, loss = 0.17669462\n",
      "Iteration 1141, loss = 0.17661688\n",
      "Iteration 1142, loss = 0.17642789\n",
      "Iteration 1143, loss = 0.17634396\n",
      "Iteration 1144, loss = 0.17615436\n",
      "Iteration 1145, loss = 0.17601727\n",
      "Iteration 1146, loss = 0.17588815\n",
      "Iteration 1147, loss = 0.17577603\n",
      "Iteration 1148, loss = 0.17569286\n",
      "Iteration 1149, loss = 0.17550650\n",
      "Iteration 1150, loss = 0.17535677\n",
      "Iteration 1151, loss = 0.17521535\n",
      "Iteration 1152, loss = 0.17512010\n",
      "Iteration 1153, loss = 0.17501733\n",
      "Iteration 1154, loss = 0.17482367\n",
      "Iteration 1155, loss = 0.17469321\n",
      "Iteration 1156, loss = 0.17456422\n",
      "Iteration 1157, loss = 0.17447479\n",
      "Iteration 1158, loss = 0.17439039\n",
      "Iteration 1159, loss = 0.17432595\n",
      "Iteration 1160, loss = 0.17403795\n",
      "Iteration 1161, loss = 0.17400413\n",
      "Iteration 1162, loss = 0.17379336\n",
      "Iteration 1163, loss = 0.17379072\n",
      "Iteration 1164, loss = 0.17363907\n",
      "Iteration 1165, loss = 0.17347976\n",
      "Iteration 1166, loss = 0.17329026\n",
      "Iteration 1167, loss = 0.17316437\n",
      "Iteration 1168, loss = 0.17303876\n",
      "Iteration 1169, loss = 0.17297271\n",
      "Iteration 1170, loss = 0.17283111\n",
      "Iteration 1171, loss = 0.17267883\n",
      "Iteration 1172, loss = 0.17255278\n",
      "Iteration 1173, loss = 0.17239642\n",
      "Iteration 1174, loss = 0.17227131\n",
      "Iteration 1175, loss = 0.17224188\n",
      "Iteration 1176, loss = 0.17204276\n",
      "Iteration 1177, loss = 0.17189291\n",
      "Iteration 1178, loss = 0.17176704\n",
      "Iteration 1179, loss = 0.17180529\n",
      "Iteration 1180, loss = 0.17152100\n",
      "Iteration 1181, loss = 0.17140255\n",
      "Iteration 1182, loss = 0.17126971\n",
      "Iteration 1183, loss = 0.17125301\n",
      "Iteration 1184, loss = 0.17106896\n",
      "Iteration 1185, loss = 0.17092508\n",
      "Iteration 1186, loss = 0.17079683\n",
      "Iteration 1187, loss = 0.17066069\n",
      "Iteration 1188, loss = 0.17053365\n",
      "Iteration 1189, loss = 0.17042077\n",
      "Iteration 1190, loss = 0.17043361\n",
      "Iteration 1191, loss = 0.17023025\n",
      "Iteration 1192, loss = 0.17007932\n",
      "Iteration 1193, loss = 0.16995808\n",
      "Iteration 1194, loss = 0.16985932\n",
      "Iteration 1195, loss = 0.16975176\n",
      "Iteration 1196, loss = 0.16957452\n",
      "Iteration 1197, loss = 0.16951880\n",
      "Iteration 1198, loss = 0.16940399\n",
      "Iteration 1199, loss = 0.16921606\n",
      "Iteration 1200, loss = 0.16910496\n",
      "Iteration 1201, loss = 0.16898540\n",
      "Iteration 1202, loss = 0.16892618\n",
      "Iteration 1203, loss = 0.16874632\n",
      "Iteration 1204, loss = 0.16862883\n",
      "Iteration 1205, loss = 0.16855760\n",
      "Iteration 1206, loss = 0.16840758\n",
      "Iteration 1207, loss = 0.16830664\n",
      "Iteration 1208, loss = 0.16819805\n",
      "Iteration 1209, loss = 0.16806206\n",
      "Iteration 1210, loss = 0.16794430\n",
      "Iteration 1211, loss = 0.16781743\n",
      "Iteration 1212, loss = 0.16772753\n",
      "Iteration 1213, loss = 0.16759410\n",
      "Iteration 1214, loss = 0.16746683\n",
      "Iteration 1215, loss = 0.16736476\n",
      "Iteration 1216, loss = 0.16722152\n",
      "Iteration 1217, loss = 0.16711878\n",
      "Iteration 1218, loss = 0.16698082\n",
      "Iteration 1219, loss = 0.16687963\n",
      "Iteration 1220, loss = 0.16678114\n",
      "Iteration 1221, loss = 0.16666798\n",
      "Iteration 1222, loss = 0.16659006\n",
      "Iteration 1223, loss = 0.16643322\n",
      "Iteration 1224, loss = 0.16632607\n",
      "Iteration 1225, loss = 0.16620371\n",
      "Iteration 1226, loss = 0.16606521\n",
      "Iteration 1227, loss = 0.16596643\n",
      "Iteration 1228, loss = 0.16583859\n",
      "Iteration 1229, loss = 0.16575809\n",
      "Iteration 1230, loss = 0.16562711\n",
      "Iteration 1231, loss = 0.16560365\n",
      "Iteration 1232, loss = 0.16546170\n",
      "Iteration 1233, loss = 0.16529402\n",
      "Iteration 1234, loss = 0.16533489\n",
      "Iteration 1235, loss = 0.16506574\n",
      "Iteration 1236, loss = 0.16495767\n",
      "Iteration 1237, loss = 0.16480859\n",
      "Iteration 1238, loss = 0.16471359\n",
      "Iteration 1239, loss = 0.16465077\n",
      "Iteration 1240, loss = 0.16451179\n",
      "Iteration 1241, loss = 0.16437331\n",
      "Iteration 1242, loss = 0.16428600\n",
      "Iteration 1243, loss = 0.16415417\n",
      "Iteration 1244, loss = 0.16408336\n",
      "Iteration 1245, loss = 0.16395607\n",
      "Iteration 1246, loss = 0.16391265\n",
      "Iteration 1247, loss = 0.16375394\n",
      "Iteration 1248, loss = 0.16367680\n",
      "Iteration 1249, loss = 0.16350428\n",
      "Iteration 1250, loss = 0.16338627\n",
      "Iteration 1251, loss = 0.16336533\n",
      "Iteration 1252, loss = 0.16316583\n",
      "Iteration 1253, loss = 0.16307292\n",
      "Iteration 1254, loss = 0.16294476\n",
      "Iteration 1255, loss = 0.16287471\n",
      "Iteration 1256, loss = 0.16272590\n",
      "Iteration 1257, loss = 0.16263047\n",
      "Iteration 1258, loss = 0.16253356\n",
      "Iteration 1259, loss = 0.16240205\n",
      "Iteration 1260, loss = 0.16233435\n",
      "Iteration 1261, loss = 0.16218480\n",
      "Iteration 1262, loss = 0.16209189\n",
      "Iteration 1263, loss = 0.16198199\n",
      "Iteration 1264, loss = 0.16185466\n",
      "Iteration 1265, loss = 0.16180032\n",
      "Iteration 1266, loss = 0.16178444\n",
      "Iteration 1267, loss = 0.16155784\n",
      "Iteration 1268, loss = 0.16147106\n",
      "Iteration 1269, loss = 0.16141533\n",
      "Iteration 1270, loss = 0.16123498\n",
      "Iteration 1271, loss = 0.16115651\n",
      "Iteration 1272, loss = 0.16100512\n",
      "Iteration 1273, loss = 0.16098938\n",
      "Iteration 1274, loss = 0.16083341\n",
      "Iteration 1275, loss = 0.16082430\n",
      "Iteration 1276, loss = 0.16058998\n",
      "Iteration 1277, loss = 0.16049141\n",
      "Iteration 1278, loss = 0.16042143\n",
      "Iteration 1279, loss = 0.16027635\n",
      "Iteration 1280, loss = 0.16018783\n",
      "Iteration 1281, loss = 0.16011726\n",
      "Iteration 1282, loss = 0.16001967\n",
      "Iteration 1283, loss = 0.15998648\n",
      "Iteration 1284, loss = 0.15979354\n",
      "Iteration 1285, loss = 0.15969480\n",
      "Iteration 1286, loss = 0.15956894\n",
      "Iteration 1287, loss = 0.15953353\n",
      "Iteration 1288, loss = 0.15936336\n",
      "Iteration 1289, loss = 0.15928568\n",
      "Iteration 1290, loss = 0.15915829\n",
      "Iteration 1291, loss = 0.15904870\n",
      "Iteration 1292, loss = 0.15898661\n",
      "Iteration 1293, loss = 0.15888365\n",
      "Iteration 1294, loss = 0.15881369\n",
      "Iteration 1295, loss = 0.15866975\n",
      "Iteration 1296, loss = 0.15853793\n",
      "Iteration 1297, loss = 0.15848821\n",
      "Iteration 1298, loss = 0.15834393\n",
      "Iteration 1299, loss = 0.15823237\n",
      "Iteration 1300, loss = 0.15814886\n",
      "Iteration 1301, loss = 0.15804329\n",
      "Iteration 1302, loss = 0.15794043\n",
      "Iteration 1303, loss = 0.15790614\n",
      "Iteration 1304, loss = 0.15775791\n",
      "Iteration 1305, loss = 0.15767400\n",
      "Iteration 1306, loss = 0.15754231\n",
      "Iteration 1307, loss = 0.15746622\n",
      "Iteration 1308, loss = 0.15742548\n",
      "Iteration 1309, loss = 0.15726962\n",
      "Iteration 1310, loss = 0.15714017\n",
      "Iteration 1311, loss = 0.15705300\n",
      "Iteration 1312, loss = 0.15693788\n",
      "Iteration 1313, loss = 0.15683548\n",
      "Iteration 1314, loss = 0.15679885\n",
      "Iteration 1315, loss = 0.15663885\n",
      "Iteration 1316, loss = 0.15657135\n",
      "Iteration 1317, loss = 0.15645053\n",
      "Iteration 1318, loss = 0.15635080\n",
      "Iteration 1319, loss = 0.15628019\n",
      "Iteration 1320, loss = 0.15619402\n",
      "Iteration 1321, loss = 0.15611075\n",
      "Iteration 1322, loss = 0.15600150\n",
      "Iteration 1323, loss = 0.15585203\n",
      "Iteration 1324, loss = 0.15579121\n",
      "Iteration 1325, loss = 0.15567381\n",
      "Iteration 1326, loss = 0.15557171\n",
      "Iteration 1327, loss = 0.15549615\n",
      "Iteration 1328, loss = 0.15540914\n",
      "Iteration 1329, loss = 0.15530630\n",
      "Iteration 1330, loss = 0.15522797\n",
      "Iteration 1331, loss = 0.15509729\n",
      "Iteration 1332, loss = 0.15503776\n",
      "Iteration 1333, loss = 0.15491106\n",
      "Iteration 1334, loss = 0.15486984\n",
      "Iteration 1335, loss = 0.15478652\n",
      "Iteration 1336, loss = 0.15461514\n",
      "Iteration 1337, loss = 0.15456499\n",
      "Iteration 1338, loss = 0.15445125\n",
      "Iteration 1339, loss = 0.15434415\n",
      "Iteration 1340, loss = 0.15440189\n",
      "Iteration 1341, loss = 0.15415047\n",
      "Iteration 1342, loss = 0.15405259\n",
      "Iteration 1343, loss = 0.15399552\n",
      "Iteration 1344, loss = 0.15387649\n",
      "Iteration 1345, loss = 0.15378991\n",
      "Iteration 1346, loss = 0.15370132\n",
      "Iteration 1347, loss = 0.15370486\n",
      "Iteration 1348, loss = 0.15352126\n",
      "Iteration 1349, loss = 0.15339418\n",
      "Iteration 1350, loss = 0.15337976\n",
      "Iteration 1351, loss = 0.15317885\n",
      "Iteration 1352, loss = 0.15309573\n",
      "Iteration 1353, loss = 0.15301836\n",
      "Iteration 1354, loss = 0.15292311\n",
      "Iteration 1355, loss = 0.15287673\n",
      "Iteration 1356, loss = 0.15273120\n",
      "Iteration 1357, loss = 0.15266165\n",
      "Iteration 1358, loss = 0.15259408\n",
      "Iteration 1359, loss = 0.15252991\n",
      "Iteration 1360, loss = 0.15244523\n",
      "Iteration 1361, loss = 0.15237553\n",
      "Iteration 1362, loss = 0.15219600\n",
      "Iteration 1363, loss = 0.15208876\n",
      "Iteration 1364, loss = 0.15202463\n",
      "Iteration 1365, loss = 0.15197034\n",
      "Iteration 1366, loss = 0.15181611\n",
      "Iteration 1367, loss = 0.15182160\n",
      "Iteration 1368, loss = 0.15176244\n",
      "Iteration 1369, loss = 0.15165532\n",
      "Iteration 1370, loss = 0.15153088\n",
      "Iteration 1371, loss = 0.15140160\n",
      "Iteration 1372, loss = 0.15129770\n",
      "Iteration 1373, loss = 0.15118504\n",
      "Iteration 1374, loss = 0.15111182\n",
      "Iteration 1375, loss = 0.15104958\n",
      "Iteration 1376, loss = 0.15091058\n",
      "Iteration 1377, loss = 0.15093924\n",
      "Iteration 1378, loss = 0.15077013\n",
      "Iteration 1379, loss = 0.15066442\n",
      "Iteration 1380, loss = 0.15058614\n",
      "Iteration 1381, loss = 0.15054342\n",
      "Iteration 1382, loss = 0.15038009\n",
      "Iteration 1383, loss = 0.15030078\n",
      "Iteration 1384, loss = 0.15021255\n",
      "Iteration 1385, loss = 0.15013527\n",
      "Iteration 1386, loss = 0.15019450\n",
      "Iteration 1387, loss = 0.15012220\n",
      "Iteration 1388, loss = 0.14988678\n",
      "Iteration 1389, loss = 0.14995848\n",
      "Iteration 1390, loss = 0.14969210\n",
      "Iteration 1391, loss = 0.14959184\n",
      "Iteration 1392, loss = 0.14950603\n",
      "Iteration 1393, loss = 0.14943890\n",
      "Iteration 1394, loss = 0.14937623\n",
      "Iteration 1395, loss = 0.14924201\n",
      "Iteration 1396, loss = 0.14918686\n",
      "Iteration 1397, loss = 0.14912261\n",
      "Iteration 1398, loss = 0.14900909\n",
      "Iteration 1399, loss = 0.14889062\n",
      "Iteration 1400, loss = 0.14885906\n",
      "Iteration 1401, loss = 0.14871574\n",
      "Iteration 1402, loss = 0.14868124\n",
      "Iteration 1403, loss = 0.14854764\n",
      "Iteration 1404, loss = 0.14859000\n",
      "Iteration 1405, loss = 0.14841005\n",
      "Iteration 1406, loss = 0.14829536\n",
      "Iteration 1407, loss = 0.14823638\n",
      "Iteration 1408, loss = 0.14813982\n",
      "Iteration 1409, loss = 0.14805239\n",
      "Iteration 1410, loss = 0.14798164\n",
      "Iteration 1411, loss = 0.14792886\n",
      "Iteration 1412, loss = 0.14779608\n",
      "Iteration 1413, loss = 0.14770600\n",
      "Iteration 1414, loss = 0.14776172\n",
      "Iteration 1415, loss = 0.14754596\n",
      "Iteration 1416, loss = 0.14743738\n",
      "Iteration 1417, loss = 0.14737878\n",
      "Iteration 1418, loss = 0.14729639\n",
      "Iteration 1419, loss = 0.14719349\n",
      "Iteration 1420, loss = 0.14722305\n",
      "Iteration 1421, loss = 0.14706654\n",
      "Iteration 1422, loss = 0.14698960\n",
      "Iteration 1423, loss = 0.14694755\n",
      "Iteration 1424, loss = 0.14679282\n",
      "Iteration 1425, loss = 0.14675996\n",
      "Iteration 1426, loss = 0.14665607\n",
      "Iteration 1427, loss = 0.14656147\n",
      "Iteration 1428, loss = 0.14645590\n",
      "Iteration 1429, loss = 0.14651886\n",
      "Iteration 1430, loss = 0.14629250\n",
      "Iteration 1431, loss = 0.14630864\n",
      "Iteration 1432, loss = 0.14613395\n",
      "Iteration 1433, loss = 0.14605247\n",
      "Iteration 1434, loss = 0.14595186\n",
      "Iteration 1435, loss = 0.14588046\n",
      "Iteration 1436, loss = 0.14584059\n",
      "Iteration 1437, loss = 0.14571919\n",
      "Iteration 1438, loss = 0.14563197\n",
      "Iteration 1439, loss = 0.14558347\n",
      "Iteration 1440, loss = 0.14549982\n",
      "Iteration 1441, loss = 0.14539915\n",
      "Iteration 1442, loss = 0.14533947\n",
      "Iteration 1443, loss = 0.14522474\n",
      "Iteration 1444, loss = 0.14515629\n",
      "Iteration 1445, loss = 0.14510809\n",
      "Iteration 1446, loss = 0.14501049\n",
      "Iteration 1447, loss = 0.14513447\n",
      "Iteration 1448, loss = 0.14489559\n",
      "Iteration 1449, loss = 0.14474993\n",
      "Iteration 1450, loss = 0.14469581\n",
      "Iteration 1451, loss = 0.14462007\n",
      "Iteration 1452, loss = 0.14454389\n",
      "Iteration 1453, loss = 0.14461183\n",
      "Iteration 1454, loss = 0.14436582\n",
      "Iteration 1455, loss = 0.14427638\n",
      "Iteration 1456, loss = 0.14419946\n",
      "Iteration 1457, loss = 0.14415271\n",
      "Iteration 1458, loss = 0.14411397\n",
      "Iteration 1459, loss = 0.14400383\n",
      "Iteration 1460, loss = 0.14391576\n",
      "Iteration 1461, loss = 0.14381207\n",
      "Iteration 1462, loss = 0.14376342\n",
      "Iteration 1463, loss = 0.14366839\n",
      "Iteration 1464, loss = 0.14366935\n",
      "Iteration 1465, loss = 0.14349224\n",
      "Iteration 1466, loss = 0.14343376\n",
      "Iteration 1467, loss = 0.14334548\n",
      "Iteration 1468, loss = 0.14336607\n",
      "Iteration 1469, loss = 0.14345193\n",
      "Iteration 1470, loss = 0.14310879\n",
      "Iteration 1471, loss = 0.14304945\n",
      "Iteration 1472, loss = 0.14293610\n",
      "Iteration 1473, loss = 0.14294895\n",
      "Iteration 1474, loss = 0.14283209\n",
      "Iteration 1475, loss = 0.14283237\n",
      "Iteration 1476, loss = 0.14272278\n",
      "Iteration 1477, loss = 0.14260074\n",
      "Iteration 1478, loss = 0.14257259\n",
      "Iteration 1479, loss = 0.14248036\n",
      "Iteration 1480, loss = 0.14237992\n",
      "Iteration 1481, loss = 0.14231108\n",
      "Iteration 1482, loss = 0.14227154\n",
      "Iteration 1483, loss = 0.14218495\n",
      "Iteration 1484, loss = 0.14205290\n",
      "Iteration 1485, loss = 0.14198084\n",
      "Iteration 1486, loss = 0.14193220\n",
      "Iteration 1487, loss = 0.14196597\n",
      "Iteration 1488, loss = 0.14179634\n",
      "Iteration 1489, loss = 0.14168757\n",
      "Iteration 1490, loss = 0.14162217\n",
      "Iteration 1491, loss = 0.14169415\n",
      "Iteration 1492, loss = 0.14148253\n",
      "Iteration 1493, loss = 0.14137277\n",
      "Iteration 1494, loss = 0.14131400\n",
      "Iteration 1495, loss = 0.14127934\n",
      "Iteration 1496, loss = 0.14118028\n",
      "Iteration 1497, loss = 0.14110427\n",
      "Iteration 1498, loss = 0.14103281\n",
      "Iteration 1499, loss = 0.14095624\n",
      "Iteration 1500, loss = 0.14091470\n",
      "Iteration 1501, loss = 0.14082210\n",
      "Iteration 1502, loss = 0.14076072\n",
      "Iteration 1503, loss = 0.14065650\n",
      "Iteration 1504, loss = 0.14067252\n",
      "Iteration 1505, loss = 0.14050906\n",
      "Iteration 1506, loss = 0.14042979\n",
      "Iteration 1507, loss = 0.14040728\n",
      "Iteration 1508, loss = 0.14031468\n",
      "Iteration 1509, loss = 0.14028860\n",
      "Iteration 1510, loss = 0.14017315\n",
      "Iteration 1511, loss = 0.14006832\n",
      "Iteration 1512, loss = 0.14000284\n",
      "Iteration 1513, loss = 0.13994462\n",
      "Iteration 1514, loss = 0.13987367\n",
      "Iteration 1515, loss = 0.13978839\n",
      "Iteration 1516, loss = 0.13983298\n",
      "Iteration 1517, loss = 0.13975503\n",
      "Iteration 1518, loss = 0.13962815\n",
      "Iteration 1519, loss = 0.13950510\n",
      "Iteration 1520, loss = 0.13948938\n",
      "Iteration 1521, loss = 0.13940978\n",
      "Iteration 1522, loss = 0.13940145\n",
      "Iteration 1523, loss = 0.13922373\n",
      "Iteration 1524, loss = 0.13919504\n",
      "Iteration 1525, loss = 0.13907116\n",
      "Iteration 1526, loss = 0.13927071\n",
      "Iteration 1527, loss = 0.13914716\n",
      "Iteration 1528, loss = 0.13888965\n",
      "Iteration 1529, loss = 0.13878170\n",
      "Iteration 1530, loss = 0.13874473\n",
      "Iteration 1531, loss = 0.13880397\n",
      "Iteration 1532, loss = 0.13865874\n",
      "Iteration 1533, loss = 0.13860579\n",
      "Iteration 1534, loss = 0.13856140\n",
      "Iteration 1535, loss = 0.13846429\n",
      "Iteration 1536, loss = 0.13837011\n",
      "Iteration 1537, loss = 0.13831566\n",
      "Iteration 1538, loss = 0.13819420\n",
      "Iteration 1539, loss = 0.13815921\n",
      "Iteration 1540, loss = 0.13809838\n",
      "Iteration 1541, loss = 0.13802173\n",
      "Iteration 1542, loss = 0.13793993\n",
      "Iteration 1543, loss = 0.13788589\n",
      "Iteration 1544, loss = 0.13780590\n",
      "Iteration 1545, loss = 0.13771807\n",
      "Iteration 1546, loss = 0.13769077\n",
      "Iteration 1547, loss = 0.13764938\n",
      "Iteration 1548, loss = 0.13749765\n",
      "Iteration 1549, loss = 0.13745681\n",
      "Iteration 1550, loss = 0.13740654\n",
      "Iteration 1551, loss = 0.13739489\n",
      "Iteration 1552, loss = 0.13729570\n",
      "Iteration 1553, loss = 0.13720507\n",
      "Iteration 1554, loss = 0.13712509\n",
      "Iteration 1555, loss = 0.13729099\n",
      "Iteration 1556, loss = 0.13703968\n",
      "Iteration 1557, loss = 0.13706117\n",
      "Iteration 1558, loss = 0.13697363\n",
      "Iteration 1559, loss = 0.13683488\n",
      "Iteration 1560, loss = 0.13675066\n",
      "Iteration 1561, loss = 0.13667586\n",
      "Iteration 1562, loss = 0.13658125\n",
      "Iteration 1563, loss = 0.13656475\n",
      "Iteration 1564, loss = 0.13647514\n",
      "Iteration 1565, loss = 0.13639997\n",
      "Iteration 1566, loss = 0.13633338\n",
      "Iteration 1567, loss = 0.13627541\n",
      "Iteration 1568, loss = 0.13619064\n",
      "Iteration 1569, loss = 0.13614901\n",
      "Iteration 1570, loss = 0.13615364\n",
      "Iteration 1571, loss = 0.13601495\n",
      "Iteration 1572, loss = 0.13594397\n",
      "Iteration 1573, loss = 0.13596310\n",
      "Iteration 1574, loss = 0.13591037\n",
      "Iteration 1575, loss = 0.13576983\n",
      "Iteration 1576, loss = 0.13568090\n",
      "Iteration 1577, loss = 0.13564154\n",
      "Iteration 1578, loss = 0.13556370\n",
      "Iteration 1579, loss = 0.13549528\n",
      "Iteration 1580, loss = 0.13547056\n",
      "Iteration 1581, loss = 0.13538507\n",
      "Iteration 1582, loss = 0.13530591\n",
      "Iteration 1583, loss = 0.13523163\n",
      "Iteration 1584, loss = 0.13519477\n",
      "Iteration 1585, loss = 0.13516781\n",
      "Iteration 1586, loss = 0.13546236\n",
      "Iteration 1587, loss = 0.13498691\n",
      "Iteration 1588, loss = 0.13498105\n",
      "Iteration 1589, loss = 0.13490152\n",
      "Iteration 1590, loss = 0.13485746\n",
      "Iteration 1591, loss = 0.13481137\n",
      "Iteration 1592, loss = 0.13470794\n",
      "Iteration 1593, loss = 0.13462937\n",
      "Iteration 1594, loss = 0.13456963\n",
      "Iteration 1595, loss = 0.13449034\n",
      "Iteration 1596, loss = 0.13448778\n",
      "Iteration 1597, loss = 0.13438045\n",
      "Iteration 1598, loss = 0.13434023\n",
      "Iteration 1599, loss = 0.13424271\n",
      "Iteration 1600, loss = 0.13426493\n",
      "Iteration 1601, loss = 0.13415361\n",
      "Iteration 1602, loss = 0.13408636\n",
      "Iteration 1603, loss = 0.13403249\n",
      "Iteration 1604, loss = 0.13399926\n",
      "Iteration 1605, loss = 0.13399649\n",
      "Iteration 1606, loss = 0.13381896\n",
      "Iteration 1607, loss = 0.13379349\n",
      "Iteration 1608, loss = 0.13369680\n",
      "Iteration 1609, loss = 0.13363852\n",
      "Iteration 1610, loss = 0.13360802\n",
      "Iteration 1611, loss = 0.13354659\n",
      "Iteration 1612, loss = 0.13351339\n",
      "Iteration 1613, loss = 0.13347906\n",
      "Iteration 1614, loss = 0.13335307\n",
      "Iteration 1615, loss = 0.13331472\n",
      "Iteration 1616, loss = 0.13322215\n",
      "Iteration 1617, loss = 0.13316190\n",
      "Iteration 1618, loss = 0.13318159\n",
      "Iteration 1619, loss = 0.13304736\n",
      "Iteration 1620, loss = 0.13298131\n",
      "Iteration 1621, loss = 0.13303887\n",
      "Iteration 1622, loss = 0.13287202\n",
      "Iteration 1623, loss = 0.13279398\n",
      "Iteration 1624, loss = 0.13275291\n",
      "Iteration 1625, loss = 0.13268898\n",
      "Iteration 1626, loss = 0.13264706\n",
      "Iteration 1627, loss = 0.13267865\n",
      "Iteration 1628, loss = 0.13258121\n",
      "Iteration 1629, loss = 0.13248811\n",
      "Iteration 1630, loss = 0.13261426\n",
      "Iteration 1631, loss = 0.13232217\n",
      "Iteration 1632, loss = 0.13229578\n",
      "Iteration 1633, loss = 0.13227175\n",
      "Iteration 1634, loss = 0.13229503\n",
      "Iteration 1635, loss = 0.13211233\n",
      "Iteration 1636, loss = 0.13202927\n",
      "Iteration 1637, loss = 0.13199707\n",
      "Iteration 1638, loss = 0.13197013\n",
      "Iteration 1639, loss = 0.13189818\n",
      "Iteration 1640, loss = 0.13182728\n",
      "Iteration 1641, loss = 0.13178387\n",
      "Iteration 1642, loss = 0.13171208\n",
      "Iteration 1643, loss = 0.13163968\n",
      "Iteration 1644, loss = 0.13164902\n",
      "Iteration 1645, loss = 0.13162525\n",
      "Iteration 1646, loss = 0.13154579\n",
      "Iteration 1647, loss = 0.13152705\n",
      "Iteration 1648, loss = 0.13138699\n",
      "Iteration 1649, loss = 0.13129972\n",
      "Iteration 1650, loss = 0.13125211\n",
      "Iteration 1651, loss = 0.13135631\n",
      "Iteration 1652, loss = 0.13125575\n",
      "Iteration 1653, loss = 0.13109411\n",
      "Iteration 1654, loss = 0.13105953\n",
      "Iteration 1655, loss = 0.13101141\n",
      "Iteration 1656, loss = 0.13092264\n",
      "Iteration 1657, loss = 0.13090124\n",
      "Iteration 1658, loss = 0.13098190\n",
      "Iteration 1659, loss = 0.13089370\n",
      "Iteration 1660, loss = 0.13069898\n",
      "Iteration 1661, loss = 0.13062616\n",
      "Iteration 1662, loss = 0.13057325\n",
      "Iteration 1663, loss = 0.13052100\n",
      "Iteration 1664, loss = 0.13055089\n",
      "Iteration 1665, loss = 0.13043729\n",
      "Iteration 1666, loss = 0.13040410\n",
      "Iteration 1667, loss = 0.13034672\n",
      "Iteration 1668, loss = 0.13033510\n",
      "Iteration 1669, loss = 0.13019031\n",
      "Iteration 1670, loss = 0.13016034\n",
      "Iteration 1671, loss = 0.13010675\n",
      "Iteration 1672, loss = 0.13006107\n",
      "Iteration 1673, loss = 0.13002772\n",
      "Iteration 1674, loss = 0.12991713\n",
      "Iteration 1675, loss = 0.12988029\n",
      "Iteration 1676, loss = 0.12992439\n",
      "Iteration 1677, loss = 0.12977297\n",
      "Iteration 1678, loss = 0.12978448\n",
      "Iteration 1679, loss = 0.12965676\n",
      "Iteration 1680, loss = 0.12959235\n",
      "Iteration 1681, loss = 0.12956218\n",
      "Iteration 1682, loss = 0.12950321\n",
      "Iteration 1683, loss = 0.12948552\n",
      "Iteration 1684, loss = 0.12941611\n",
      "Iteration 1685, loss = 0.12933958\n",
      "Iteration 1686, loss = 0.12933662\n",
      "Iteration 1687, loss = 0.12926021\n",
      "Iteration 1688, loss = 0.12916893\n",
      "Iteration 1689, loss = 0.12922004\n",
      "Iteration 1690, loss = 0.12909587\n",
      "Iteration 1691, loss = 0.12905081\n",
      "Iteration 1692, loss = 0.12901225\n",
      "Iteration 1693, loss = 0.12892244\n",
      "Iteration 1694, loss = 0.12893620\n",
      "Iteration 1695, loss = 0.12884616\n",
      "Iteration 1696, loss = 0.12876868\n",
      "Iteration 1697, loss = 0.12872333\n",
      "Iteration 1698, loss = 0.12885707\n",
      "Iteration 1699, loss = 0.12859789\n",
      "Iteration 1700, loss = 0.12861354\n",
      "Iteration 1701, loss = 0.12849982\n",
      "Iteration 1702, loss = 0.12869424\n",
      "Iteration 1703, loss = 0.12846044\n",
      "Iteration 1704, loss = 0.12838046\n",
      "Iteration 1705, loss = 0.12829851\n",
      "Iteration 1706, loss = 0.12824098\n",
      "Iteration 1707, loss = 0.12821432\n",
      "Iteration 1708, loss = 0.12818423\n",
      "Iteration 1709, loss = 0.12813333\n",
      "Iteration 1710, loss = 0.12808901\n",
      "Iteration 1711, loss = 0.12804276\n",
      "Iteration 1712, loss = 0.12797544\n",
      "Iteration 1713, loss = 0.12788406\n",
      "Iteration 1714, loss = 0.12783624\n",
      "Iteration 1715, loss = 0.12784895\n",
      "Iteration 1716, loss = 0.12785999\n",
      "Iteration 1717, loss = 0.12775942\n",
      "Iteration 1718, loss = 0.12765045\n",
      "Iteration 1719, loss = 0.12759122\n",
      "Iteration 1720, loss = 0.12759296\n",
      "Iteration 1721, loss = 0.12754668\n",
      "Iteration 1722, loss = 0.12747035\n",
      "Iteration 1723, loss = 0.12737714\n",
      "Iteration 1724, loss = 0.12734955\n",
      "Iteration 1725, loss = 0.12734722\n",
      "Iteration 1726, loss = 0.12728094\n",
      "Iteration 1727, loss = 0.12729022\n",
      "Iteration 1728, loss = 0.12723706\n",
      "Iteration 1729, loss = 0.12713137\n",
      "Iteration 1730, loss = 0.12707229\n",
      "Iteration 1731, loss = 0.12715641\n",
      "Iteration 1732, loss = 0.12712027\n",
      "Iteration 1733, loss = 0.12695616\n",
      "Iteration 1734, loss = 0.12702480\n",
      "Iteration 1735, loss = 0.12685196\n",
      "Iteration 1736, loss = 0.12679137\n",
      "Iteration 1737, loss = 0.12677113\n",
      "Iteration 1738, loss = 0.12673670\n",
      "Iteration 1739, loss = 0.12661613\n",
      "Iteration 1740, loss = 0.12662863\n",
      "Iteration 1741, loss = 0.12667246\n",
      "Iteration 1742, loss = 0.12647688\n",
      "Iteration 1743, loss = 0.12644180\n",
      "Iteration 1744, loss = 0.12642989\n",
      "Iteration 1745, loss = 0.12634549\n",
      "Iteration 1746, loss = 0.12629501\n",
      "Iteration 1747, loss = 0.12626454\n",
      "Iteration 1748, loss = 0.12619309\n",
      "Iteration 1749, loss = 0.12619279\n",
      "Iteration 1750, loss = 0.12613359\n",
      "Iteration 1751, loss = 0.12620471\n",
      "Iteration 1752, loss = 0.12606620\n",
      "Iteration 1753, loss = 0.12596984\n",
      "Iteration 1754, loss = 0.12590590\n",
      "Iteration 1755, loss = 0.12591711\n",
      "Iteration 1756, loss = 0.12584729\n",
      "Iteration 1757, loss = 0.12575848\n",
      "Iteration 1758, loss = 0.12574386\n",
      "Iteration 1759, loss = 0.12568508\n",
      "Iteration 1760, loss = 0.12567629\n",
      "Iteration 1761, loss = 0.12566111\n",
      "Iteration 1762, loss = 0.12556075\n",
      "Iteration 1763, loss = 0.12555630\n",
      "Iteration 1764, loss = 0.12552730\n",
      "Iteration 1765, loss = 0.12542941\n",
      "Iteration 1766, loss = 0.12565601\n",
      "Iteration 1767, loss = 0.12547429\n",
      "Iteration 1768, loss = 0.12536953\n",
      "Iteration 1769, loss = 0.12521220\n",
      "Iteration 1770, loss = 0.12536433\n",
      "Iteration 1771, loss = 0.12523824\n",
      "Iteration 1772, loss = 0.12512848\n",
      "Iteration 1773, loss = 0.12503865\n",
      "Iteration 1774, loss = 0.12502598\n",
      "Iteration 1775, loss = 0.12515989\n",
      "Iteration 1776, loss = 0.12495496\n",
      "Iteration 1777, loss = 0.12498258\n",
      "Iteration 1778, loss = 0.12487909\n",
      "Iteration 1779, loss = 0.12482571\n",
      "Iteration 1780, loss = 0.12476506\n",
      "Iteration 1781, loss = 0.12467670\n",
      "Iteration 1782, loss = 0.12465534\n",
      "Iteration 1783, loss = 0.12460542\n",
      "Iteration 1784, loss = 0.12456704\n",
      "Iteration 1785, loss = 0.12459885\n",
      "Iteration 1786, loss = 0.12447014\n",
      "Iteration 1787, loss = 0.12442283\n",
      "Iteration 1788, loss = 0.12460136\n",
      "Iteration 1789, loss = 0.12437014\n",
      "Iteration 1790, loss = 0.12430147\n",
      "Iteration 1791, loss = 0.12434082\n",
      "Iteration 1792, loss = 0.12422127\n",
      "Iteration 1793, loss = 0.12443566\n",
      "Iteration 1794, loss = 0.12411834\n",
      "Iteration 1795, loss = 0.12418477\n",
      "Iteration 1796, loss = 0.12407942\n",
      "Iteration 1797, loss = 0.12417117\n",
      "Iteration 1798, loss = 0.12394905\n",
      "Iteration 1799, loss = 0.12391761\n",
      "Iteration 1800, loss = 0.12387183\n",
      "Iteration 1801, loss = 0.12382469\n",
      "Iteration 1802, loss = 0.12377989\n",
      "Iteration 1803, loss = 0.12379442\n",
      "Iteration 1804, loss = 0.12370100\n",
      "Iteration 1805, loss = 0.12398992\n",
      "Iteration 1806, loss = 0.12379155\n",
      "Iteration 1807, loss = 0.12356768\n",
      "Iteration 1808, loss = 0.12361277\n",
      "Iteration 1809, loss = 0.12352494\n",
      "Iteration 1810, loss = 0.12346500\n",
      "Iteration 1811, loss = 0.12368045\n",
      "Iteration 1812, loss = 0.12340352\n",
      "Iteration 1813, loss = 0.12337049\n",
      "Iteration 1814, loss = 0.12339510\n",
      "Iteration 1815, loss = 0.12325354\n",
      "Iteration 1816, loss = 0.12344269\n",
      "Iteration 1817, loss = 0.12318398\n",
      "Iteration 1818, loss = 0.12321356\n",
      "Iteration 1819, loss = 0.12316566\n",
      "Iteration 1820, loss = 0.12307261\n",
      "Iteration 1821, loss = 0.12299281\n",
      "Iteration 1822, loss = 0.12301576\n",
      "Iteration 1823, loss = 0.12294754\n",
      "Iteration 1824, loss = 0.12295305\n",
      "Iteration 1825, loss = 0.12290249\n",
      "Iteration 1826, loss = 0.12284571\n",
      "Iteration 1827, loss = 0.12286086\n",
      "Iteration 1828, loss = 0.12272383\n",
      "Iteration 1829, loss = 0.12276786\n",
      "Iteration 1830, loss = 0.12270316\n",
      "Iteration 1831, loss = 0.12263142\n",
      "Iteration 1832, loss = 0.12262157\n",
      "Iteration 1833, loss = 0.12263044\n",
      "Iteration 1834, loss = 0.12248768\n",
      "Iteration 1835, loss = 0.12252319\n",
      "Iteration 1836, loss = 0.12241363\n",
      "Iteration 1837, loss = 0.12242520\n",
      "Iteration 1838, loss = 0.12234062\n",
      "Iteration 1839, loss = 0.12230294\n",
      "Iteration 1840, loss = 0.12227739\n",
      "Iteration 1841, loss = 0.12225230\n",
      "Iteration 1842, loss = 0.12229411\n",
      "Iteration 1843, loss = 0.12237000\n",
      "Iteration 1844, loss = 0.12212765\n",
      "Iteration 1845, loss = 0.12211223\n",
      "Iteration 1846, loss = 0.12202330\n",
      "Iteration 1847, loss = 0.12199326\n",
      "Iteration 1848, loss = 0.12198887\n",
      "Iteration 1849, loss = 0.12197529\n",
      "Iteration 1850, loss = 0.12203060\n",
      "Iteration 1851, loss = 0.12196264\n",
      "Iteration 1852, loss = 0.12186879\n",
      "Iteration 1853, loss = 0.12180483\n",
      "Iteration 1854, loss = 0.12177789\n",
      "Iteration 1855, loss = 0.12182049\n",
      "Iteration 1856, loss = 0.12166562\n",
      "Iteration 1857, loss = 0.12162330\n",
      "Iteration 1858, loss = 0.12160270\n",
      "Iteration 1859, loss = 0.12161147\n",
      "Iteration 1860, loss = 0.12158865\n",
      "Iteration 1861, loss = 0.12156996\n",
      "Iteration 1862, loss = 0.12148647\n",
      "Iteration 1863, loss = 0.12139225\n",
      "Iteration 1864, loss = 0.12136877\n",
      "Iteration 1865, loss = 0.12132970\n",
      "Iteration 1866, loss = 0.12138511\n",
      "Iteration 1867, loss = 0.12126617\n",
      "Iteration 1868, loss = 0.12119935\n",
      "Iteration 1869, loss = 0.12116623\n",
      "Iteration 1870, loss = 0.12122480\n",
      "Iteration 1871, loss = 0.12118065\n",
      "Iteration 1872, loss = 0.12141087\n",
      "Iteration 1873, loss = 0.12103272\n",
      "Iteration 1874, loss = 0.12098388\n",
      "Iteration 1875, loss = 0.12100333\n",
      "Iteration 1876, loss = 0.12097866\n",
      "Iteration 1877, loss = 0.12097037\n",
      "Iteration 1878, loss = 0.12084094\n",
      "Iteration 1879, loss = 0.12083171\n",
      "Iteration 1880, loss = 0.12081687\n",
      "Iteration 1881, loss = 0.12080043\n",
      "Iteration 1882, loss = 0.12075137\n",
      "Iteration 1883, loss = 0.12083221\n",
      "Iteration 1884, loss = 0.12066964\n",
      "Iteration 1885, loss = 0.12065566\n",
      "Iteration 1886, loss = 0.12060605\n",
      "Iteration 1887, loss = 0.12053495\n",
      "Iteration 1888, loss = 0.12061559\n",
      "Iteration 1889, loss = 0.12047696\n",
      "Iteration 1890, loss = 0.12042604\n",
      "Iteration 1891, loss = 0.12047255\n",
      "Iteration 1892, loss = 0.12039448\n",
      "Iteration 1893, loss = 0.12033033\n",
      "Iteration 1894, loss = 0.12029728\n",
      "Iteration 1895, loss = 0.12052383\n",
      "Iteration 1896, loss = 0.12028100\n",
      "Iteration 1897, loss = 0.12023160\n",
      "Iteration 1898, loss = 0.12020050\n",
      "Iteration 1899, loss = 0.12017313\n",
      "Iteration 1900, loss = 0.12012210\n",
      "Iteration 1901, loss = 0.12011242\n",
      "Iteration 1902, loss = 0.12002355\n",
      "Iteration 1903, loss = 0.11998079\n",
      "Iteration 1904, loss = 0.11997684\n",
      "Iteration 1905, loss = 0.11996895\n",
      "Iteration 1906, loss = 0.11992862\n",
      "Iteration 1907, loss = 0.12003706\n",
      "Iteration 1908, loss = 0.11986574\n",
      "Iteration 1909, loss = 0.11979417\n",
      "Iteration 1910, loss = 0.11980147\n",
      "Iteration 1911, loss = 0.11980471\n",
      "Iteration 1912, loss = 0.11969561\n",
      "Iteration 1913, loss = 0.11967236\n",
      "Iteration 1914, loss = 0.11964323\n",
      "Iteration 1915, loss = 0.11960907\n",
      "Iteration 1916, loss = 0.11978706\n",
      "Iteration 1917, loss = 0.11959726\n",
      "Iteration 1918, loss = 0.11953469\n",
      "Iteration 1919, loss = 0.11949210\n",
      "Iteration 1920, loss = 0.11958675\n",
      "Iteration 1921, loss = 0.11956974\n",
      "Iteration 1922, loss = 0.11935717\n",
      "Iteration 1923, loss = 0.11933686\n",
      "Iteration 1924, loss = 0.11935316\n",
      "Iteration 1925, loss = 0.11928327\n",
      "Iteration 1926, loss = 0.11920384\n",
      "Iteration 1927, loss = 0.11921975\n",
      "Iteration 1928, loss = 0.11917955\n",
      "Iteration 1929, loss = 0.11914855\n",
      "Iteration 1930, loss = 0.11909149\n",
      "Iteration 1931, loss = 0.11909052\n",
      "Iteration 1932, loss = 0.11923755\n",
      "Iteration 1933, loss = 0.11900658\n",
      "Iteration 1934, loss = 0.11904666\n",
      "Iteration 1935, loss = 0.11892947\n",
      "Iteration 1936, loss = 0.11889761\n",
      "Iteration 1937, loss = 0.11900023\n",
      "Iteration 1938, loss = 0.11895762\n",
      "Iteration 1939, loss = 0.11909418\n",
      "Iteration 1940, loss = 0.11881937\n",
      "Iteration 1941, loss = 0.11878864\n",
      "Iteration 1942, loss = 0.11875416\n",
      "Iteration 1943, loss = 0.11875053\n",
      "Iteration 1944, loss = 0.11865393\n",
      "Iteration 1945, loss = 0.11863419\n",
      "Iteration 1946, loss = 0.11858511\n",
      "Iteration 1947, loss = 0.11860019\n",
      "Iteration 1948, loss = 0.11855995\n",
      "Iteration 1949, loss = 0.11852983\n",
      "Iteration 1950, loss = 0.11846760\n",
      "Iteration 1951, loss = 0.11848766\n",
      "Iteration 1952, loss = 0.11844165\n",
      "Iteration 1953, loss = 0.11843999\n",
      "Iteration 1954, loss = 0.11838558\n",
      "Iteration 1955, loss = 0.11836889\n",
      "Iteration 1956, loss = 0.11828743\n",
      "Iteration 1957, loss = 0.11823969\n",
      "Iteration 1958, loss = 0.11828847\n",
      "Iteration 1959, loss = 0.11821185\n",
      "Iteration 1960, loss = 0.11819100\n",
      "Iteration 1961, loss = 0.11824018\n",
      "Iteration 1962, loss = 0.11811591\n",
      "Iteration 1963, loss = 0.11810408\n",
      "Iteration 1964, loss = 0.11805720\n",
      "Iteration 1965, loss = 0.11801356\n",
      "Iteration 1966, loss = 0.11797481\n",
      "Iteration 1967, loss = 0.11796342\n",
      "Iteration 1968, loss = 0.11791226\n",
      "Iteration 1969, loss = 0.11789703\n",
      "Iteration 1970, loss = 0.11788925\n",
      "Iteration 1971, loss = 0.11784610\n",
      "Iteration 1972, loss = 0.11779700\n",
      "Iteration 1973, loss = 0.11780432\n",
      "Iteration 1974, loss = 0.11778703\n",
      "Iteration 1975, loss = 0.11771693\n",
      "Iteration 1976, loss = 0.11780686\n",
      "Iteration 1977, loss = 0.11771933\n",
      "Iteration 1978, loss = 0.11763206\n",
      "Iteration 1979, loss = 0.11760037\n",
      "Iteration 1980, loss = 0.11755485\n",
      "Iteration 1981, loss = 0.11757365\n",
      "Iteration 1982, loss = 0.11777032\n",
      "Iteration 1983, loss = 0.11748625\n",
      "Iteration 1984, loss = 0.11750404\n",
      "Iteration 1985, loss = 0.11745850\n",
      "Iteration 1986, loss = 0.11747179\n",
      "Iteration 1987, loss = 0.11738581\n",
      "Iteration 1988, loss = 0.11739249\n",
      "Iteration 1989, loss = 0.11729406\n",
      "Iteration 1990, loss = 0.11739464\n",
      "Iteration 1991, loss = 0.11722412\n",
      "Iteration 1992, loss = 0.11725371\n",
      "Iteration 1993, loss = 0.11718831\n",
      "Iteration 1994, loss = 0.11723824\n",
      "Iteration 1995, loss = 0.11716402\n",
      "Iteration 1996, loss = 0.11721716\n",
      "Iteration 1997, loss = 0.11710821\n",
      "Iteration 1998, loss = 0.11704939\n",
      "Iteration 1999, loss = 0.11728406\n",
      "Iteration 2000, loss = 0.11717144\n",
      "Iteration 2001, loss = 0.11697617\n",
      "Iteration 2002, loss = 0.11703386\n",
      "Iteration 2003, loss = 0.11693373\n",
      "Iteration 2004, loss = 0.11691212\n",
      "Iteration 2005, loss = 0.11687893\n",
      "Iteration 2006, loss = 0.11692532\n",
      "Iteration 2007, loss = 0.11678525\n",
      "Iteration 2008, loss = 0.11676215\n",
      "Iteration 2009, loss = 0.11676804\n",
      "Iteration 2010, loss = 0.11689568\n",
      "Iteration 2011, loss = 0.11678738\n",
      "Iteration 2012, loss = 0.11671319\n",
      "Iteration 2013, loss = 0.11662399\n",
      "Iteration 2014, loss = 0.11663624\n",
      "Iteration 2015, loss = 0.11662179\n",
      "Iteration 2016, loss = 0.11666330\n",
      "Iteration 2017, loss = 0.11655914\n",
      "Iteration 2018, loss = 0.11656410\n",
      "Iteration 2019, loss = 0.11651245\n",
      "Iteration 2020, loss = 0.11643117\n",
      "Iteration 2021, loss = 0.11649906\n",
      "Iteration 2022, loss = 0.11650126\n",
      "Iteration 2023, loss = 0.11636514\n",
      "Iteration 2024, loss = 0.11650385\n",
      "Iteration 2025, loss = 0.11629883\n",
      "Iteration 2026, loss = 0.11637018\n",
      "Iteration 2027, loss = 0.11635831\n",
      "Iteration 2028, loss = 0.11625957\n",
      "Iteration 2029, loss = 0.11638092\n",
      "Iteration 2030, loss = 0.11642966\n",
      "Iteration 2031, loss = 0.11616941\n",
      "Iteration 2032, loss = 0.11616509\n",
      "Iteration 2033, loss = 0.11621947\n",
      "Iteration 2034, loss = 0.11608864\n",
      "Iteration 2035, loss = 0.11606305\n",
      "Iteration 2036, loss = 0.11604667\n",
      "Iteration 2037, loss = 0.11606821\n",
      "Iteration 2038, loss = 0.11600483\n",
      "Iteration 2039, loss = 0.11601617\n",
      "Iteration 2040, loss = 0.11592596\n",
      "Iteration 2041, loss = 0.11591728\n",
      "Iteration 2042, loss = 0.11593411\n",
      "Iteration 2043, loss = 0.11591485\n",
      "Iteration 2044, loss = 0.11586085\n",
      "Iteration 2045, loss = 0.11580714\n",
      "Iteration 2046, loss = 0.11576134\n",
      "Iteration 2047, loss = 0.11577915\n",
      "Iteration 2048, loss = 0.11592783\n",
      "Iteration 2049, loss = 0.11581085\n",
      "Iteration 2050, loss = 0.11566593\n",
      "Iteration 2051, loss = 0.11563201\n",
      "Iteration 2052, loss = 0.11561021\n",
      "Iteration 2053, loss = 0.11568259\n",
      "Iteration 2054, loss = 0.11560798\n",
      "Iteration 2055, loss = 0.11570794\n",
      "Iteration 2056, loss = 0.11572015\n",
      "Iteration 2057, loss = 0.11562626\n",
      "Iteration 2058, loss = 0.11563288\n",
      "Iteration 2059, loss = 0.11546245\n",
      "Iteration 2060, loss = 0.11545611\n",
      "Iteration 2061, loss = 0.11557425\n",
      "Iteration 2062, loss = 0.11538091\n",
      "Iteration 2063, loss = 0.11531105\n",
      "Iteration 2064, loss = 0.11529392\n",
      "Iteration 2065, loss = 0.11533428\n",
      "Iteration 2066, loss = 0.11536227\n",
      "Iteration 2067, loss = 0.11528311\n",
      "Iteration 2068, loss = 0.11526763\n",
      "Iteration 2069, loss = 0.11520209\n",
      "Iteration 2070, loss = 0.11520395\n",
      "Iteration 2071, loss = 0.11536666\n",
      "Iteration 2072, loss = 0.11513320\n",
      "Iteration 2073, loss = 0.11515725\n",
      "Iteration 2074, loss = 0.11511602\n",
      "Iteration 2075, loss = 0.11522367\n",
      "Iteration 2076, loss = 0.11508340\n",
      "Iteration 2077, loss = 0.11501166\n",
      "Iteration 2078, loss = 0.11502606\n",
      "Iteration 2079, loss = 0.11508231\n",
      "Iteration 2080, loss = 0.11496727\n",
      "Iteration 2081, loss = 0.11493617\n",
      "Iteration 2082, loss = 0.11490410\n",
      "Iteration 2083, loss = 0.11491971\n",
      "Iteration 2084, loss = 0.11484888\n",
      "Iteration 2085, loss = 0.11483344\n",
      "Iteration 2086, loss = 0.11481132\n",
      "Iteration 2087, loss = 0.11479597\n",
      "Iteration 2088, loss = 0.11474294\n",
      "Iteration 2089, loss = 0.11476464\n",
      "Iteration 2090, loss = 0.11471253\n",
      "Iteration 2091, loss = 0.11475486\n",
      "Iteration 2092, loss = 0.11466717\n",
      "Iteration 2093, loss = 0.11478205\n",
      "Iteration 2094, loss = 0.11460997\n",
      "Iteration 2095, loss = 0.11458654\n",
      "Iteration 2096, loss = 0.11467092\n",
      "Iteration 2097, loss = 0.11466078\n",
      "Iteration 2098, loss = 0.11452745\n",
      "Iteration 2099, loss = 0.11459566\n",
      "Iteration 2100, loss = 0.11454258\n",
      "Iteration 2101, loss = 0.11455186\n",
      "Iteration 2102, loss = 0.11446186\n",
      "Iteration 2103, loss = 0.11446682\n",
      "Iteration 2104, loss = 0.11466089\n",
      "Iteration 2105, loss = 0.11436769\n",
      "Iteration 2106, loss = 0.11435918\n",
      "Iteration 2107, loss = 0.11435701\n",
      "Iteration 2108, loss = 0.11428423\n",
      "Iteration 2109, loss = 0.11437561\n",
      "Iteration 2110, loss = 0.11428073\n",
      "Iteration 2111, loss = 0.11438728\n",
      "Iteration 2112, loss = 0.11446347\n",
      "Iteration 2113, loss = 0.11418587\n",
      "Iteration 2114, loss = 0.11418309\n",
      "Iteration 2115, loss = 0.11416316\n",
      "Iteration 2116, loss = 0.11429355\n",
      "Iteration 2117, loss = 0.11429020\n",
      "Iteration 2118, loss = 0.11416174\n",
      "Iteration 2119, loss = 0.11404940\n",
      "Iteration 2120, loss = 0.11417717\n",
      "Iteration 2121, loss = 0.11401805\n",
      "Iteration 2122, loss = 0.11404117\n",
      "Iteration 2123, loss = 0.11414085\n",
      "Iteration 2124, loss = 0.11424631\n",
      "Iteration 2125, loss = 0.11402584\n",
      "Iteration 2126, loss = 0.11410990\n",
      "Iteration 2127, loss = 0.11391348\n",
      "Iteration 2128, loss = 0.11388921\n",
      "Iteration 2129, loss = 0.11386656\n",
      "Iteration 2130, loss = 0.11383420\n",
      "Iteration 2131, loss = 0.11387420\n",
      "Iteration 2132, loss = 0.11379722\n",
      "Iteration 2133, loss = 0.11389241\n",
      "Iteration 2134, loss = 0.11387311\n",
      "Iteration 2135, loss = 0.11420166\n",
      "Iteration 2136, loss = 0.11374316\n",
      "Iteration 2137, loss = 0.11367888\n",
      "Iteration 2138, loss = 0.11366819\n",
      "Iteration 2139, loss = 0.11363422\n",
      "Iteration 2140, loss = 0.11375853\n",
      "Iteration 2141, loss = 0.11363657\n",
      "Iteration 2142, loss = 0.11357732\n",
      "Iteration 2143, loss = 0.11359705\n",
      "Iteration 2144, loss = 0.11353487\n",
      "Iteration 2145, loss = 0.11351387\n",
      "Iteration 2146, loss = 0.11356530\n",
      "Iteration 2147, loss = 0.11350668\n",
      "Iteration 2148, loss = 0.11348608\n",
      "Iteration 2149, loss = 0.11349832\n",
      "Iteration 2150, loss = 0.11349064\n",
      "Iteration 2151, loss = 0.11339601\n",
      "Iteration 2152, loss = 0.11355991\n",
      "Iteration 2153, loss = 0.11337933\n",
      "Iteration 2154, loss = 0.11336662\n",
      "Iteration 2155, loss = 0.11340139\n",
      "Iteration 2156, loss = 0.11330539\n",
      "Iteration 2157, loss = 0.11330450\n",
      "Iteration 2158, loss = 0.11350294\n",
      "Iteration 2159, loss = 0.11332304\n",
      "Iteration 2160, loss = 0.11319739\n",
      "Iteration 2161, loss = 0.11320612\n",
      "Iteration 2162, loss = 0.11319327\n",
      "Iteration 2163, loss = 0.11319830\n",
      "Iteration 2164, loss = 0.11311866\n",
      "Iteration 2165, loss = 0.11339361\n",
      "Iteration 2166, loss = 0.11329743\n",
      "Iteration 2167, loss = 0.11309697\n",
      "Iteration 2168, loss = 0.11318203\n",
      "Iteration 2169, loss = 0.11337363\n",
      "Iteration 2170, loss = 0.11312421\n",
      "Iteration 2171, loss = 0.11310513\n",
      "Iteration 2172, loss = 0.11296968\n",
      "Iteration 2173, loss = 0.11294538\n",
      "Iteration 2174, loss = 0.11292565\n",
      "Iteration 2175, loss = 0.11291887\n",
      "Iteration 2176, loss = 0.11292036\n",
      "Iteration 2177, loss = 0.11287847\n",
      "Iteration 2178, loss = 0.11288528\n",
      "Iteration 2179, loss = 0.11284118\n",
      "Iteration 2180, loss = 0.11283874\n",
      "Iteration 2181, loss = 0.11285503\n",
      "Iteration 2182, loss = 0.11281805\n",
      "Iteration 2183, loss = 0.11305538\n",
      "Iteration 2184, loss = 0.11277758\n",
      "Iteration 2185, loss = 0.11281101\n",
      "Iteration 2186, loss = 0.11270317\n",
      "Iteration 2187, loss = 0.11267185\n",
      "Iteration 2188, loss = 0.11266077\n",
      "Iteration 2189, loss = 0.11266937\n",
      "Iteration 2190, loss = 0.11265733\n",
      "Iteration 2191, loss = 0.11279206\n",
      "Iteration 2192, loss = 0.11260753\n",
      "Iteration 2193, loss = 0.11263892\n",
      "Iteration 2194, loss = 0.11263694\n",
      "Iteration 2195, loss = 0.11256318\n",
      "Iteration 2196, loss = 0.11258703\n",
      "Iteration 2197, loss = 0.11255079\n",
      "Iteration 2198, loss = 0.11251520\n",
      "Iteration 2199, loss = 0.11248515\n",
      "Iteration 2200, loss = 0.11249093\n",
      "Iteration 2201, loss = 0.11242645\n",
      "Iteration 2202, loss = 0.11267295\n",
      "Iteration 2203, loss = 0.11239593\n",
      "Iteration 2204, loss = 0.11275651\n",
      "Iteration 2205, loss = 0.11243764\n",
      "Iteration 2206, loss = 0.11245696\n",
      "Iteration 2207, loss = 0.11231524\n",
      "Iteration 2208, loss = 0.11235658\n",
      "Iteration 2209, loss = 0.11230523\n",
      "Iteration 2210, loss = 0.11239219\n",
      "Iteration 2211, loss = 0.11227752\n",
      "Iteration 2212, loss = 0.11235959\n",
      "Iteration 2213, loss = 0.11225039\n",
      "Iteration 2214, loss = 0.11219921\n",
      "Iteration 2215, loss = 0.11217056\n",
      "Iteration 2216, loss = 0.11221410\n",
      "Iteration 2217, loss = 0.11218062\n",
      "Iteration 2218, loss = 0.11215821\n",
      "Iteration 2219, loss = 0.11211703\n",
      "Iteration 2220, loss = 0.11209991\n",
      "Iteration 2221, loss = 0.11213692\n",
      "Iteration 2222, loss = 0.11207815\n",
      "Iteration 2223, loss = 0.11203635\n",
      "Iteration 2224, loss = 0.11203230\n",
      "Iteration 2225, loss = 0.11208322\n",
      "Iteration 2226, loss = 0.11201740\n",
      "Iteration 2227, loss = 0.11198361\n",
      "Iteration 2228, loss = 0.11196916\n",
      "Iteration 2229, loss = 0.11195985\n",
      "Iteration 2230, loss = 0.11197858\n",
      "Iteration 2231, loss = 0.11197569\n",
      "Iteration 2232, loss = 0.11189553\n",
      "Iteration 2233, loss = 0.11185677\n",
      "Iteration 2234, loss = 0.11186341\n",
      "Iteration 2235, loss = 0.11204377\n",
      "Iteration 2236, loss = 0.11192930\n",
      "Iteration 2237, loss = 0.11181117\n",
      "Iteration 2238, loss = 0.11187186\n",
      "Iteration 2239, loss = 0.11189643\n",
      "Iteration 2240, loss = 0.11175463\n",
      "Iteration 2241, loss = 0.11171331\n",
      "Iteration 2242, loss = 0.11170243\n",
      "Iteration 2243, loss = 0.11176103\n",
      "Iteration 2244, loss = 0.11167724\n",
      "Iteration 2245, loss = 0.11177502\n",
      "Iteration 2246, loss = 0.11166263\n",
      "Iteration 2247, loss = 0.11166419\n",
      "Iteration 2248, loss = 0.11167822\n",
      "Iteration 2249, loss = 0.11159454\n",
      "Iteration 2250, loss = 0.11161557\n",
      "Iteration 2251, loss = 0.11176946\n",
      "Iteration 2252, loss = 0.11164192\n",
      "Iteration 2253, loss = 0.11153606\n",
      "Iteration 2254, loss = 0.11156276\n",
      "Iteration 2255, loss = 0.11152300\n",
      "Iteration 2256, loss = 0.11170469\n",
      "Iteration 2257, loss = 0.11147910\n",
      "Iteration 2258, loss = 0.11156912\n",
      "Iteration 2259, loss = 0.11162229\n",
      "Iteration 2260, loss = 0.11142052\n",
      "Iteration 2261, loss = 0.11155449\n",
      "Iteration 2262, loss = 0.11152640\n",
      "Iteration 2263, loss = 0.11138484\n",
      "Iteration 2264, loss = 0.11149283\n",
      "Iteration 2265, loss = 0.11134942\n",
      "Iteration 2266, loss = 0.11147689\n",
      "Iteration 2267, loss = 0.11131718\n",
      "Iteration 2268, loss = 0.11132890\n",
      "Iteration 2269, loss = 0.11138054\n",
      "Iteration 2270, loss = 0.11126051\n",
      "Iteration 2271, loss = 0.11158013\n",
      "Iteration 2272, loss = 0.11125644\n",
      "Iteration 2273, loss = 0.11129954\n",
      "Iteration 2274, loss = 0.11121743\n",
      "Iteration 2275, loss = 0.11121972\n",
      "Iteration 2276, loss = 0.11116955\n",
      "Iteration 2277, loss = 0.11124656\n",
      "Iteration 2278, loss = 0.11119719\n",
      "Iteration 2279, loss = 0.11114136\n",
      "Iteration 2280, loss = 0.11112042\n",
      "Iteration 2281, loss = 0.11111041\n",
      "Iteration 2282, loss = 0.11108091\n",
      "Iteration 2283, loss = 0.11104714\n",
      "Iteration 2284, loss = 0.11106449\n",
      "Iteration 2285, loss = 0.11102217\n",
      "Iteration 2286, loss = 0.11102479\n",
      "Iteration 2287, loss = 0.11103577\n",
      "Iteration 2288, loss = 0.11106317\n",
      "Iteration 2289, loss = 0.11103978\n",
      "Iteration 2290, loss = 0.11107769\n",
      "Iteration 2291, loss = 0.11101619\n",
      "Iteration 2292, loss = 0.11091621\n",
      "Iteration 2293, loss = 0.11089017\n",
      "Iteration 2294, loss = 0.11093142\n",
      "Iteration 2295, loss = 0.11106397\n",
      "Iteration 2296, loss = 0.11085341\n",
      "Iteration 2297, loss = 0.11085265\n",
      "Iteration 2298, loss = 0.11094025\n",
      "Iteration 2299, loss = 0.11083522\n",
      "Iteration 2300, loss = 0.11090111\n",
      "Iteration 2301, loss = 0.11082535\n",
      "Iteration 2302, loss = 0.11086151\n",
      "Iteration 2303, loss = 0.11080043\n",
      "Iteration 2304, loss = 0.11078446\n",
      "Iteration 2305, loss = 0.11072408\n",
      "Iteration 2306, loss = 0.11069791\n",
      "Iteration 2307, loss = 0.11076606\n",
      "Iteration 2308, loss = 0.11072785\n",
      "Iteration 2309, loss = 0.11078169\n",
      "Iteration 2310, loss = 0.11064935\n",
      "Iteration 2311, loss = 0.11081045\n",
      "Iteration 2312, loss = 0.11066330\n",
      "Iteration 2313, loss = 0.11074232\n",
      "Iteration 2314, loss = 0.11064550\n",
      "Iteration 2315, loss = 0.11060903\n",
      "Iteration 2316, loss = 0.11070900\n",
      "Iteration 2317, loss = 0.11057467\n",
      "Iteration 2318, loss = 0.11057657\n",
      "Iteration 2319, loss = 0.11052187\n",
      "Iteration 2320, loss = 0.11053431\n",
      "Iteration 2321, loss = 0.11050445\n",
      "Iteration 2322, loss = 0.11057218\n",
      "Iteration 2323, loss = 0.11052528\n",
      "Iteration 2324, loss = 0.11047801\n",
      "Iteration 2325, loss = 0.11043482\n",
      "Iteration 2326, loss = 0.11043158\n",
      "Iteration 2327, loss = 0.11041556\n",
      "Iteration 2328, loss = 0.11042094\n",
      "Iteration 2329, loss = 0.11045443\n",
      "Iteration 2330, loss = 0.11053241\n",
      "Iteration 2331, loss = 0.11037016\n",
      "Iteration 2332, loss = 0.11037971\n",
      "Iteration 2333, loss = 0.11048481\n",
      "Iteration 2334, loss = 0.11044477\n",
      "Iteration 2335, loss = 0.11030743\n",
      "Iteration 2336, loss = 0.11030307\n",
      "Iteration 2337, loss = 0.11028936\n",
      "Iteration 2338, loss = 0.11026368\n",
      "Iteration 2339, loss = 0.11044076\n",
      "Iteration 2340, loss = 0.11034262\n",
      "Iteration 2341, loss = 0.11023106\n",
      "Iteration 2342, loss = 0.11023458\n",
      "Iteration 2343, loss = 0.11019874\n",
      "Iteration 2344, loss = 0.11023124\n",
      "Iteration 2345, loss = 0.11025900\n",
      "Iteration 2346, loss = 0.11018492\n",
      "Iteration 2347, loss = 0.11018163\n",
      "Iteration 2348, loss = 0.11013411\n",
      "Iteration 2349, loss = 0.11011264\n",
      "Iteration 2350, loss = 0.11008608\n",
      "Iteration 2351, loss = 0.11013429\n",
      "Iteration 2352, loss = 0.11014202\n",
      "Iteration 2353, loss = 0.11018727\n",
      "Iteration 2354, loss = 0.11006179\n",
      "Iteration 2355, loss = 0.11000098\n",
      "Iteration 2356, loss = 0.10999983\n",
      "Iteration 2357, loss = 0.11004964\n",
      "Iteration 2358, loss = 0.11006640\n",
      "Iteration 2359, loss = 0.11017423\n",
      "Iteration 2360, loss = 0.10995662\n",
      "Iteration 2361, loss = 0.11001281\n",
      "Iteration 2362, loss = 0.10994001\n",
      "Iteration 2363, loss = 0.10997649\n",
      "Iteration 2364, loss = 0.10988664\n",
      "Iteration 2365, loss = 0.10995949\n",
      "Iteration 2366, loss = 0.10992725\n",
      "Iteration 2367, loss = 0.10987689\n",
      "Iteration 2368, loss = 0.11003045\n",
      "Iteration 2369, loss = 0.10982720\n",
      "Iteration 2370, loss = 0.10983771\n",
      "Iteration 2371, loss = 0.10982953\n",
      "Iteration 2372, loss = 0.10980270\n",
      "Iteration 2373, loss = 0.11006518\n",
      "Iteration 2374, loss = 0.10982878\n",
      "Iteration 2375, loss = 0.10974279\n",
      "Iteration 2376, loss = 0.10981510\n",
      "Iteration 2377, loss = 0.10973850\n",
      "Iteration 2378, loss = 0.11001190\n",
      "Iteration 2379, loss = 0.10971495\n",
      "Iteration 2380, loss = 0.10975913\n",
      "Iteration 2381, loss = 0.10985261\n",
      "Iteration 2382, loss = 0.10970695\n",
      "Iteration 2383, loss = 0.11001668\n",
      "Iteration 2384, loss = 0.10969733\n",
      "Iteration 2385, loss = 0.10975609\n",
      "Iteration 2386, loss = 0.10967469\n",
      "Iteration 2387, loss = 0.10963358\n",
      "Iteration 2388, loss = 0.10960974\n",
      "Iteration 2389, loss = 0.10962090\n",
      "Iteration 2390, loss = 0.10969054\n",
      "Iteration 2391, loss = 0.10958123\n",
      "Iteration 2392, loss = 0.10954313\n",
      "Iteration 2393, loss = 0.10959876\n",
      "Iteration 2394, loss = 0.10956346\n",
      "Iteration 2395, loss = 0.10955643\n",
      "Iteration 2396, loss = 0.10954067\n",
      "Iteration 2397, loss = 0.10965370\n",
      "Iteration 2398, loss = 0.10955992\n",
      "Iteration 2399, loss = 0.10953612\n",
      "Iteration 2400, loss = 0.10947019\n",
      "Iteration 2401, loss = 0.10947812\n",
      "Iteration 2402, loss = 0.10942418\n",
      "Iteration 2403, loss = 0.10945397\n",
      "Iteration 2404, loss = 0.10955645\n",
      "Iteration 2405, loss = 0.10936678\n",
      "Iteration 2406, loss = 0.10948187\n",
      "Iteration 2407, loss = 0.10941155\n",
      "Iteration 2408, loss = 0.10938709\n",
      "Iteration 2409, loss = 0.10935552\n",
      "Iteration 2410, loss = 0.10940261\n",
      "Iteration 2411, loss = 0.10934966\n",
      "Iteration 2412, loss = 0.10937501\n",
      "Iteration 2413, loss = 0.10930095\n",
      "Iteration 2414, loss = 0.10931984\n",
      "Iteration 2415, loss = 0.10926321\n",
      "Iteration 2416, loss = 0.10932029\n",
      "Iteration 2417, loss = 0.10934420\n",
      "Iteration 2418, loss = 0.10928563\n",
      "Iteration 2419, loss = 0.10931668\n",
      "Iteration 2420, loss = 0.10927031\n",
      "Iteration 2421, loss = 0.10926628\n",
      "Iteration 2422, loss = 0.10924470\n",
      "Iteration 2423, loss = 0.10921483\n",
      "Iteration 2424, loss = 0.10925510\n",
      "Iteration 2425, loss = 0.10921976\n",
      "Iteration 2426, loss = 0.10916943\n",
      "Iteration 2427, loss = 0.10913165\n",
      "Iteration 2428, loss = 0.10918031\n",
      "Iteration 2429, loss = 0.10910395\n",
      "Iteration 2430, loss = 0.10933340\n",
      "Iteration 2431, loss = 0.10912286\n",
      "Iteration 2432, loss = 0.10906328\n",
      "Iteration 2433, loss = 0.10910836\n",
      "Iteration 2434, loss = 0.10909411\n",
      "Iteration 2435, loss = 0.10906291\n",
      "Iteration 2436, loss = 0.10907558\n",
      "Iteration 2437, loss = 0.10904476\n",
      "Iteration 2438, loss = 0.10897470\n",
      "Iteration 2439, loss = 0.10910150\n",
      "Iteration 2440, loss = 0.10897159\n",
      "Iteration 2441, loss = 0.10902129\n",
      "Iteration 2442, loss = 0.10896170\n",
      "Iteration 2443, loss = 0.10902896\n",
      "Iteration 2444, loss = 0.10902871\n",
      "Iteration 2445, loss = 0.10903052\n",
      "Iteration 2446, loss = 0.10902297\n",
      "Iteration 2447, loss = 0.10891604\n",
      "Iteration 2448, loss = 0.10890582\n",
      "Iteration 2449, loss = 0.10888020\n",
      "Iteration 2450, loss = 0.10892379\n",
      "Iteration 2451, loss = 0.10889068\n",
      "Iteration 2452, loss = 0.10885898\n",
      "Iteration 2453, loss = 0.10904852\n",
      "Iteration 2454, loss = 0.10894556\n",
      "Iteration 2455, loss = 0.10886424\n",
      "Iteration 2456, loss = 0.10880731\n",
      "Iteration 2457, loss = 0.10885618\n",
      "Iteration 2458, loss = 0.10880727\n",
      "Iteration 2459, loss = 0.10880837\n",
      "Iteration 2460, loss = 0.10878370\n",
      "Iteration 2461, loss = 0.10881346\n",
      "Iteration 2462, loss = 0.10879177\n",
      "Iteration 2463, loss = 0.10879269\n",
      "Iteration 2464, loss = 0.10874663\n",
      "Iteration 2465, loss = 0.10894533\n",
      "Iteration 2466, loss = 0.10871333\n",
      "Iteration 2467, loss = 0.10888716\n",
      "Iteration 2468, loss = 0.10883562\n",
      "Iteration 2469, loss = 0.10875616\n",
      "Iteration 2470, loss = 0.10867899\n",
      "Iteration 2471, loss = 0.10879010\n",
      "Iteration 2472, loss = 0.10868769\n",
      "Iteration 2473, loss = 0.10882534\n",
      "Iteration 2474, loss = 0.10861110\n",
      "Iteration 2475, loss = 0.10870440\n",
      "Iteration 2476, loss = 0.10858179\n",
      "Iteration 2477, loss = 0.10857052\n",
      "Iteration 2478, loss = 0.10866746\n",
      "Iteration 2479, loss = 0.10869176\n",
      "Iteration 2480, loss = 0.10867940\n",
      "Iteration 2481, loss = 0.10855640\n",
      "Iteration 2482, loss = 0.10861635\n",
      "Iteration 2483, loss = 0.10854741\n",
      "Iteration 2484, loss = 0.10856589\n",
      "Iteration 2485, loss = 0.10851384\n",
      "Iteration 2486, loss = 0.10856794\n",
      "Iteration 2487, loss = 0.10847382\n",
      "Iteration 2488, loss = 0.10882165\n",
      "Iteration 2489, loss = 0.10858960\n",
      "Iteration 2490, loss = 0.10844967\n",
      "Iteration 2491, loss = 0.10855756\n",
      "Iteration 2492, loss = 0.10843237\n",
      "Iteration 2493, loss = 0.10850061\n",
      "Iteration 2494, loss = 0.10854617\n",
      "Iteration 2495, loss = 0.10843933\n",
      "Iteration 2496, loss = 0.10840928\n",
      "Iteration 2497, loss = 0.10851447\n",
      "Iteration 2498, loss = 0.10842951\n",
      "Iteration 2499, loss = 0.10835863\n",
      "Iteration 2500, loss = 0.10838707\n",
      "Iteration 2501, loss = 0.10833262\n",
      "Iteration 2502, loss = 0.10842216\n",
      "Iteration 2503, loss = 0.10843727\n",
      "Iteration 2504, loss = 0.10832356\n",
      "Iteration 2505, loss = 0.10861027\n",
      "Iteration 2506, loss = 0.10830922\n",
      "Iteration 2507, loss = 0.10828916\n",
      "Iteration 2508, loss = 0.10840490\n",
      "Iteration 2509, loss = 0.10855752\n",
      "Iteration 2510, loss = 0.10826043\n",
      "Iteration 2511, loss = 0.10826226\n",
      "Iteration 2512, loss = 0.10824672\n",
      "Iteration 2513, loss = 0.10833239\n",
      "Iteration 2514, loss = 0.10827308\n",
      "Iteration 2515, loss = 0.10821679\n",
      "Iteration 2516, loss = 0.10820966\n",
      "Iteration 2517, loss = 0.10821240\n",
      "Iteration 2518, loss = 0.10816793\n",
      "Iteration 2519, loss = 0.10820550\n",
      "Iteration 2520, loss = 0.10814652\n",
      "Iteration 2521, loss = 0.10817219\n",
      "Iteration 2522, loss = 0.10819919\n",
      "Iteration 2523, loss = 0.10826109\n",
      "Iteration 2524, loss = 0.10837866\n",
      "Iteration 2525, loss = 0.10809177\n",
      "Iteration 2526, loss = 0.10809207\n",
      "Iteration 2527, loss = 0.10809234\n",
      "Iteration 2528, loss = 0.10806473\n",
      "Iteration 2529, loss = 0.10806758\n",
      "Iteration 2530, loss = 0.10810893\n",
      "Iteration 2531, loss = 0.10810268\n",
      "Iteration 2532, loss = 0.10804187\n",
      "Iteration 2533, loss = 0.10803153\n",
      "Iteration 2534, loss = 0.10802583\n",
      "Iteration 2535, loss = 0.10812423\n",
      "Iteration 2536, loss = 0.10818841\n",
      "Iteration 2537, loss = 0.10801745\n",
      "Iteration 2538, loss = 0.10806858\n",
      "Iteration 2539, loss = 0.10800906\n",
      "Iteration 2540, loss = 0.10794962\n",
      "Iteration 2541, loss = 0.10794020\n",
      "Iteration 2542, loss = 0.10800937\n",
      "Iteration 2543, loss = 0.10799392\n",
      "Iteration 2544, loss = 0.10797286\n",
      "Iteration 2545, loss = 0.10796231\n",
      "Iteration 2546, loss = 0.10812388\n",
      "Iteration 2547, loss = 0.10795397\n",
      "Iteration 2548, loss = 0.10803464\n",
      "Iteration 2549, loss = 0.10790689\n",
      "Iteration 2550, loss = 0.10792291\n",
      "Iteration 2551, loss = 0.10785037\n",
      "Iteration 2552, loss = 0.10786006\n",
      "Iteration 2553, loss = 0.10800330\n",
      "Iteration 2554, loss = 0.10783672\n",
      "Iteration 2555, loss = 0.10781583\n",
      "Iteration 2556, loss = 0.10784848\n",
      "Iteration 2557, loss = 0.10789295\n",
      "Iteration 2558, loss = 0.10789323\n",
      "Iteration 2559, loss = 0.10794927\n",
      "Iteration 2560, loss = 0.10786618\n",
      "Iteration 2561, loss = 0.10780592\n",
      "Iteration 2562, loss = 0.10778247\n",
      "Iteration 2563, loss = 0.10777211\n",
      "Iteration 2564, loss = 0.10778101\n",
      "Iteration 2565, loss = 0.10780529\n",
      "Iteration 2566, loss = 0.10776098\n",
      "Iteration 2567, loss = 0.10772878\n",
      "Iteration 2568, loss = 0.10769091\n",
      "Iteration 2569, loss = 0.10777604\n",
      "Iteration 2570, loss = 0.10770886\n",
      "Iteration 2571, loss = 0.10768184\n",
      "Iteration 2572, loss = 0.10775311\n",
      "Iteration 2573, loss = 0.10778236\n",
      "Iteration 2574, loss = 0.10773610\n",
      "Iteration 2575, loss = 0.10766816\n",
      "Iteration 2576, loss = 0.10764594\n",
      "Iteration 2577, loss = 0.10768325\n",
      "Iteration 2578, loss = 0.10763495\n",
      "Iteration 2579, loss = 0.10769127\n",
      "Iteration 2580, loss = 0.10760060\n",
      "Iteration 2581, loss = 0.10777763\n",
      "Iteration 2582, loss = 0.10763102\n",
      "Iteration 2583, loss = 0.10756953\n",
      "Iteration 2584, loss = 0.10758059\n",
      "Iteration 2585, loss = 0.10769815\n",
      "Iteration 2586, loss = 0.10762585\n",
      "Iteration 2587, loss = 0.10757112\n",
      "Iteration 2588, loss = 0.10764403\n",
      "Iteration 2589, loss = 0.10765197\n",
      "Iteration 2590, loss = 0.10751572\n",
      "Iteration 2591, loss = 0.10758720\n",
      "Iteration 2592, loss = 0.10752638\n",
      "Iteration 2593, loss = 0.10753233\n",
      "Iteration 2594, loss = 0.10764387\n",
      "Iteration 2595, loss = 0.10745646\n",
      "Iteration 2596, loss = 0.10744010\n",
      "Iteration 2597, loss = 0.10748762\n",
      "Iteration 2598, loss = 0.10749986\n",
      "Iteration 2599, loss = 0.10754838\n",
      "Iteration 2600, loss = 0.10745233\n",
      "Iteration 2601, loss = 0.10742309\n",
      "Iteration 2602, loss = 0.10749286\n",
      "Iteration 2603, loss = 0.10742966\n",
      "Iteration 2604, loss = 0.10778431\n",
      "Iteration 2605, loss = 0.10738791\n",
      "Iteration 2606, loss = 0.10739620\n",
      "Iteration 2607, loss = 0.10742961\n",
      "Iteration 2608, loss = 0.10736617\n",
      "Iteration 2609, loss = 0.10737646\n",
      "Iteration 2610, loss = 0.10738315\n",
      "Iteration 2611, loss = 0.10749381\n",
      "Iteration 2612, loss = 0.10733463\n",
      "Iteration 2613, loss = 0.10739051\n",
      "Iteration 2614, loss = 0.10735828\n",
      "Iteration 2615, loss = 0.10746694\n",
      "Iteration 2616, loss = 0.10728515\n",
      "Iteration 2617, loss = 0.10737449\n",
      "Iteration 2618, loss = 0.10758486\n",
      "Iteration 2619, loss = 0.10731317\n",
      "Iteration 2620, loss = 0.10731399\n",
      "Iteration 2621, loss = 0.10756696\n",
      "Iteration 2622, loss = 0.10728717\n",
      "Iteration 2623, loss = 0.10723235\n",
      "Iteration 2624, loss = 0.10741747\n",
      "Iteration 2625, loss = 0.10733955\n",
      "Iteration 2626, loss = 0.10737168\n",
      "Iteration 2627, loss = 0.10719727\n",
      "Iteration 2628, loss = 0.10723695\n",
      "Iteration 2629, loss = 0.10721689\n",
      "Iteration 2630, loss = 0.10725150\n",
      "Iteration 2631, loss = 0.10718500\n",
      "Iteration 2632, loss = 0.10718238\n",
      "Iteration 2633, loss = 0.10716808\n",
      "Iteration 2634, loss = 0.10729553\n",
      "Iteration 2635, loss = 0.10741255\n",
      "Iteration 2636, loss = 0.10716454\n",
      "Iteration 2637, loss = 0.10713397\n",
      "Iteration 2638, loss = 0.10716442\n",
      "Iteration 2639, loss = 0.10714030\n",
      "Iteration 2640, loss = 0.10715251\n",
      "Iteration 2641, loss = 0.10737599\n",
      "Iteration 2642, loss = 0.10716286\n",
      "Iteration 2643, loss = 0.10723791\n",
      "Iteration 2644, loss = 0.10714412\n",
      "Iteration 2645, loss = 0.10709789\n",
      "Iteration 2646, loss = 0.10728019\n",
      "Iteration 2647, loss = 0.10711753\n",
      "Iteration 2648, loss = 0.10706916\n",
      "Iteration 2649, loss = 0.10709132\n",
      "Iteration 2650, loss = 0.10706422\n",
      "Iteration 2651, loss = 0.10703585\n",
      "Iteration 2652, loss = 0.10701842\n",
      "Iteration 2653, loss = 0.10699580\n",
      "Iteration 2654, loss = 0.10700493\n",
      "Iteration 2655, loss = 0.10715547\n",
      "Iteration 2656, loss = 0.10702610\n",
      "Iteration 2657, loss = 0.10700046\n",
      "Iteration 2658, loss = 0.10699386\n",
      "Iteration 2659, loss = 0.10702068\n",
      "Iteration 2660, loss = 0.10698473\n",
      "Iteration 2661, loss = 0.10698590\n",
      "Iteration 2662, loss = 0.10741383\n",
      "Iteration 2663, loss = 0.10712918\n",
      "Iteration 2664, loss = 0.10690611\n",
      "Iteration 2665, loss = 0.10688817\n",
      "Iteration 2666, loss = 0.10694577\n",
      "Iteration 2667, loss = 0.10689235\n",
      "Iteration 2668, loss = 0.10691001\n",
      "Iteration 2669, loss = 0.10693688\n",
      "Iteration 2670, loss = 0.10691490\n",
      "Iteration 2671, loss = 0.10710561\n",
      "Iteration 2672, loss = 0.10698613\n",
      "Iteration 2673, loss = 0.10721301\n",
      "Iteration 2674, loss = 0.10699633\n",
      "Iteration 2675, loss = 0.10687656\n",
      "Iteration 2676, loss = 0.10692788\n",
      "Iteration 2677, loss = 0.10688716\n",
      "Iteration 2678, loss = 0.10692004\n",
      "Iteration 2679, loss = 0.10686126\n",
      "Iteration 2680, loss = 0.10692714\n",
      "Iteration 2681, loss = 0.10696377\n",
      "Iteration 2682, loss = 0.10679015\n",
      "Iteration 2683, loss = 0.10679723\n",
      "Iteration 2684, loss = 0.10683746\n",
      "Iteration 2685, loss = 0.10681617\n",
      "Iteration 2686, loss = 0.10704106\n",
      "Iteration 2687, loss = 0.10675477\n",
      "Iteration 2688, loss = 0.10709327\n",
      "Iteration 2689, loss = 0.10676856\n",
      "Iteration 2690, loss = 0.10676838\n",
      "Iteration 2691, loss = 0.10689017\n",
      "Iteration 2692, loss = 0.10683747\n",
      "Iteration 2693, loss = 0.10676110\n",
      "Iteration 2694, loss = 0.10675498\n",
      "Iteration 2695, loss = 0.10676890\n",
      "Iteration 2696, loss = 0.10673311\n",
      "Iteration 2697, loss = 0.10675643\n",
      "Iteration 2698, loss = 0.10688738\n",
      "Iteration 2699, loss = 0.10669124\n",
      "Iteration 2700, loss = 0.10680990\n",
      "Iteration 2701, loss = 0.10670755\n",
      "Iteration 2702, loss = 0.10667817\n",
      "Iteration 2703, loss = 0.10671763\n",
      "Iteration 2704, loss = 0.10662454\n",
      "Iteration 2705, loss = 0.10665423\n",
      "Iteration 2706, loss = 0.10669734\n",
      "Iteration 2707, loss = 0.10667298\n",
      "Iteration 2708, loss = 0.10667357\n",
      "Iteration 2709, loss = 0.10671991\n",
      "Iteration 2710, loss = 0.10667635\n",
      "Iteration 2711, loss = 0.10669465\n",
      "Iteration 2712, loss = 0.10689483\n",
      "Iteration 2713, loss = 0.10660813\n",
      "Iteration 2714, loss = 0.10661549\n",
      "Iteration 2715, loss = 0.10673750\n",
      "Iteration 2716, loss = 0.10665768\n",
      "Iteration 2717, loss = 0.10657979\n",
      "Iteration 2718, loss = 0.10660334\n",
      "Iteration 2719, loss = 0.10662895\n",
      "Iteration 2720, loss = 0.10658731\n",
      "Iteration 2721, loss = 0.10662977\n",
      "Iteration 2722, loss = 0.10662168\n",
      "Iteration 2723, loss = 0.10654264\n",
      "Iteration 2724, loss = 0.10653563\n",
      "Iteration 2725, loss = 0.10660904\n",
      "Iteration 2726, loss = 0.10665072\n",
      "Iteration 2727, loss = 0.10666751\n",
      "Iteration 2728, loss = 0.10659006\n",
      "Iteration 2729, loss = 0.10656349\n",
      "Iteration 2730, loss = 0.10655824\n",
      "Iteration 2731, loss = 0.10651298\n",
      "Iteration 2732, loss = 0.10651581\n",
      "Iteration 2733, loss = 0.10650883\n",
      "Iteration 2734, loss = 0.10644375\n",
      "Iteration 2735, loss = 0.10642767\n",
      "Iteration 2736, loss = 0.10647484\n",
      "Iteration 2737, loss = 0.10643133\n",
      "Iteration 2738, loss = 0.10647699\n",
      "Iteration 2739, loss = 0.10640444\n",
      "Iteration 2740, loss = 0.10642069\n",
      "Iteration 2741, loss = 0.10640159\n",
      "Iteration 2742, loss = 0.10641802\n",
      "Iteration 2743, loss = 0.10639005\n",
      "Iteration 2744, loss = 0.10642398\n",
      "Iteration 2745, loss = 0.10641865\n",
      "Iteration 2746, loss = 0.10640460\n",
      "Iteration 2747, loss = 0.10652886\n",
      "Iteration 2748, loss = 0.10644517\n",
      "Iteration 2749, loss = 0.10645658\n",
      "Iteration 2750, loss = 0.10646216\n",
      "Iteration 2751, loss = 0.10633651\n",
      "Iteration 2752, loss = 0.10637037\n",
      "Iteration 2753, loss = 0.10631445\n",
      "Iteration 2754, loss = 0.10632892\n",
      "Iteration 2755, loss = 0.10635217\n",
      "Iteration 2756, loss = 0.10639702\n",
      "Iteration 2757, loss = 0.10632804\n",
      "Iteration 2758, loss = 0.10629351\n",
      "Iteration 2759, loss = 0.10625142\n",
      "Iteration 2760, loss = 0.10631193\n",
      "Iteration 2761, loss = 0.10630870\n",
      "Iteration 2762, loss = 0.10628328\n",
      "Iteration 2763, loss = 0.10641349\n",
      "Iteration 2764, loss = 0.10625950\n",
      "Iteration 2765, loss = 0.10637229\n",
      "Iteration 2766, loss = 0.10631295\n",
      "Iteration 2767, loss = 0.10635216\n",
      "Iteration 2768, loss = 0.10626836\n",
      "Iteration 2769, loss = 0.10625008\n",
      "Iteration 2770, loss = 0.10629800\n",
      "Iteration 2771, loss = 0.10654530\n",
      "Iteration 2772, loss = 0.10621076\n",
      "Iteration 2773, loss = 0.10620149\n",
      "Iteration 2774, loss = 0.10619321\n",
      "Iteration 2775, loss = 0.10618540\n",
      "Iteration 2776, loss = 0.10623640\n",
      "Iteration 2777, loss = 0.10621539\n",
      "Iteration 2778, loss = 0.10623521\n",
      "Iteration 2779, loss = 0.10615166\n",
      "Iteration 2780, loss = 0.10623902\n",
      "Iteration 2781, loss = 0.10620339\n",
      "Iteration 2782, loss = 0.10616232\n",
      "Iteration 2783, loss = 0.10616007\n",
      "Iteration 2784, loss = 0.10619451\n",
      "Iteration 2785, loss = 0.10614535\n",
      "Iteration 2786, loss = 0.10613400\n",
      "Iteration 2787, loss = 0.10618792\n",
      "Iteration 2788, loss = 0.10610984\n",
      "Iteration 2789, loss = 0.10617666\n",
      "Iteration 2790, loss = 0.10618180\n",
      "Iteration 2791, loss = 0.10618998\n",
      "Iteration 2792, loss = 0.10611088\n",
      "Iteration 2793, loss = 0.10611413\n",
      "Iteration 2794, loss = 0.10613383\n",
      "Iteration 2795, loss = 0.10625576\n",
      "Iteration 2796, loss = 0.10607733\n",
      "Iteration 2797, loss = 0.10609561\n",
      "Iteration 2798, loss = 0.10608347\n",
      "Iteration 2799, loss = 0.10610123\n",
      "Iteration 2800, loss = 0.10609646\n",
      "Iteration 2801, loss = 0.10604514\n",
      "Iteration 2802, loss = 0.10602216\n",
      "Iteration 2803, loss = 0.10603004\n",
      "Iteration 2804, loss = 0.10619082\n",
      "Iteration 2805, loss = 0.10616895\n",
      "Iteration 2806, loss = 0.10598346\n",
      "Iteration 2807, loss = 0.10598295\n",
      "Iteration 2808, loss = 0.10604290\n",
      "Iteration 2809, loss = 0.10606232\n",
      "Iteration 2810, loss = 0.10621192\n",
      "Iteration 2811, loss = 0.10612189\n",
      "Iteration 2812, loss = 0.10597164\n",
      "Iteration 2813, loss = 0.10601245\n",
      "Iteration 2814, loss = 0.10600011\n",
      "Iteration 2815, loss = 0.10600700\n",
      "Iteration 2816, loss = 0.10597588\n",
      "Iteration 2817, loss = 0.10595432\n",
      "Iteration 2818, loss = 0.10598478\n",
      "Iteration 2819, loss = 0.10596504\n",
      "Iteration 2820, loss = 0.10592819\n",
      "Iteration 2821, loss = 0.10592754\n",
      "Iteration 2822, loss = 0.10609425\n",
      "Iteration 2823, loss = 0.10592402\n",
      "Iteration 2824, loss = 0.10614157\n",
      "Iteration 2825, loss = 0.10596687\n",
      "Iteration 2826, loss = 0.10595705\n",
      "Iteration 2827, loss = 0.10597880\n",
      "Iteration 2828, loss = 0.10596170\n",
      "Iteration 2829, loss = 0.10593377\n",
      "Iteration 2830, loss = 0.10586537\n",
      "Iteration 2831, loss = 0.10587087\n",
      "Iteration 2832, loss = 0.10590034\n",
      "Iteration 2833, loss = 0.10595750\n",
      "Iteration 2834, loss = 0.10585856\n",
      "Iteration 2835, loss = 0.10587878\n",
      "Iteration 2836, loss = 0.10590293\n",
      "Iteration 2837, loss = 0.10588573\n",
      "Iteration 2838, loss = 0.10583875\n",
      "Iteration 2839, loss = 0.10582646\n",
      "Iteration 2840, loss = 0.10581147\n",
      "Iteration 2841, loss = 0.10581427\n",
      "Iteration 2842, loss = 0.10584274\n",
      "Iteration 2843, loss = 0.10585095\n",
      "Iteration 2844, loss = 0.10635731\n",
      "Iteration 2845, loss = 0.10583312\n",
      "Iteration 2846, loss = 0.10577956\n",
      "Iteration 2847, loss = 0.10597076\n",
      "Iteration 2848, loss = 0.10582496\n",
      "Iteration 2849, loss = 0.10587274\n",
      "Iteration 2850, loss = 0.10583313\n",
      "Iteration 2851, loss = 0.10586004\n",
      "Iteration 2852, loss = 0.10576611\n",
      "Iteration 2853, loss = 0.10575838\n",
      "Iteration 2854, loss = 0.10578041\n",
      "Iteration 2855, loss = 0.10577501\n",
      "Iteration 2856, loss = 0.10586957\n",
      "Iteration 2857, loss = 0.10572076\n",
      "Iteration 2858, loss = 0.10580733\n",
      "Iteration 2859, loss = 0.10575437\n",
      "Iteration 2860, loss = 0.10573760\n",
      "Iteration 2861, loss = 0.10572229\n",
      "Iteration 2862, loss = 0.10574389\n",
      "Iteration 2863, loss = 0.10579946\n",
      "Iteration 2864, loss = 0.10574416\n",
      "Iteration 2865, loss = 0.10585898\n",
      "Iteration 2866, loss = 0.10576531\n",
      "Iteration 2867, loss = 0.10591017\n",
      "Iteration 2868, loss = 0.10595380\n",
      "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
      "\n",
      "Test results:\n",
      "  Overall score: 0.9545455\n",
      "\n",
      "000: 0 - 0, 0.0052314\n",
      "001: 0 - 0, 0.0082888\n",
      "002: 1 - 1, 0.9988171\n",
      "003: 1 - 1, 0.9970651\n",
      "004: 1 - 1, 0.9879648\n",
      "005: 0 - 0, 0.0050986\n",
      "006: 0 - 0, 0.0123388\n",
      "007: 0 - 0, 0.0553719\n",
      "008: 1 - 0, 0.0765544\n",
      "009: 1 - 1, 0.9983825\n",
      "010: 0 - 0, 0.0063437\n",
      "011: 1 - 1, 0.9964957\n",
      "012: 0 - 0, 0.0239721\n",
      "013: 0 - 0, 0.0114789\n",
      "014: 0 - 0, 0.0054699\n",
      "015: 0 - 0, 0.0239721\n",
      "016: 1 - 1, 0.9975020\n",
      "017: 1 - 1, 0.9360456\n",
      "018: 1 - 1, 0.9989389\n",
      "019: 0 - 0, 0.0406721\n",
      "020: 1 - 1, 0.9989463\n",
      "021: 1 - 1, 0.7552349\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# load the data\n",
    "xtrain= np.load(\"/home/kasia/Deep_learning/Datasets/iris2_train.npy\")\n",
    "ytrain= np.load(\"/home/kasia/Deep_learning/Datasets/iris2_train_labels.npy\")\n",
    "xtest = np.load(\"/home/kasia/Deep_learning/Datasets/iris2_test.npy\")\n",
    "ytest = np.load(\"/home/kasia/Deep_learning/Datasets/iris2_test_labels.npy\")\n",
    "\n",
    "# define the model\n",
    "clf = MLPClassifier(\n",
    "        hidden_layer_sizes=(3,2), # two hidden layers, 3 and 2 nodes each\n",
    "        activation=\"logistic\", # logistic activation - sigmoid\n",
    "        solver=\"adam\", tol=1e-9,\n",
    "        max_iter=5000,\n",
    "        verbose=True) # so we can see losses for every iteration\n",
    "clf.fit(xtrain, ytrain)\n",
    "prob = clf.predict_proba(xtest)\n",
    "score = clf.score(xtest, ytest)\n",
    "\n",
    "# get the model coefficients - weights and biases\n",
    "w12 = clf.coefs_[0]\n",
    "w23 = clf.coefs_[1]\n",
    "w34 = clf.coefs_[2]\n",
    "b1 = clf.intercepts_[0]\n",
    "b2 = clf.intercepts_[1]\n",
    "b3 = clf.intercepts_[2]\n",
    "weights = [w12,b1,w23,b2,w34,b3]\n",
    "pickle.dump(weights, open(\"/home/kasia/Deep_learning/Datasets/iris2_weights.pkl\",\"wb\"))\n",
    "\n",
    "# print probabilities and evaluate predictions\n",
    "print()\n",
    "print(\"Test results:\")\n",
    "print(\"  Overall score: %0.7f\" % score)\n",
    "print()\n",
    "for i in range(len(ytest)):\n",
    "    p = 0 if (prob[i,1] < 0.5) else 1\n",
    "    print(\"%03d: %d - %d, %0.7f\" % (i, ytest[i], p, prob[i,1]))\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199f0ba9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
